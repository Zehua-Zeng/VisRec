---
title: "time_analysis"
output: html_document
---

### Libraries required for this analysis
```{r libraries, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(fig.align="center") 
library(rstanarm)
library(tidyverse)
library(tidybayes)
library(modelr) 
library(ggplot2)
library(magrittr)  
library(emmeans)
library(bayesplot)
library(brms)
library(gganimate)

theme_set(theme_light())

source('helper_functions.R')
```

In our experiment, we used a visualization recommendation algorithm (composed of one search algorithm and one oracle algorithm) to generate visualizations for the user on one of two datasets.  We then measured the user's time to complete each of four tasks:
  1. Find Extremum
  2. Retrieve Value
  3. Prediction
  4. Exploration

Given a search algorithm (bfs or dfs), an oracle (compassql or dziban), and a dataset (birdstrikes or movies), we would like to predict the time it takes the average user to complete each task.  In addition, we would like to know if the choice of search algorithm and oracle has any meaningful impact on a user's completion time for each of these four tasks,

### Read in and clean data
```{r}
time_data = read.csv('split_by_participant_groups/completion_time.csv')

time_data <- time_data %>%
  mutate(
    dataset = as.factor(dataset),
    oracle = as.factor(oracle),
    search = as.factor(search),
    task = as.factor(task)
  )
time_data$condition <- paste(time_data$oracle, time_data$search)


task_list = c("1. Find Extremum",
              "2. Retrieve Value",
              "3. Prediction",
              "4. Exploration")

seed = 12
```

## Building a Model for Time Analysis
The prior was derived from pilot studies.  It describes the distribution of time (in seconds) needed to complete any given task.  The lognormal family was selected to prevent our model from predicting completion times of less than zero seconds. 

```{r}
model <- brm(
    formula = bf(
    completion_time ~ oracle * search + dataset + task + participant_group + (1 | participant_id)
  ),
    prior = prior(normal(360.48, 224.40), class = "Intercept"),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = time_data,
    family = lognormal(),
    file="models/time",
    seed = seed
  )   

```

### Diagnostics + Model Evaluation
In the summary table, we want to see Rhat values close to 1.0 and Bulk_ESS in the thousands.
```{r}
summary(model)
```

Trace plots help us check whether there is evidence of non-convergence for model.
```{r}
# plot(model)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).

```{r}
pairs(
  model,
  pars = c("b_Intercept",
          "b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
            "b_task3.Prediction",
            "b_task4.Exploration"),
  fixed = TRUE
)

```
Using draws from the posterior, we can visualize parameter effects and average response.  Be sure to apply an exponential transform to our log-transformed times to make it interpretable!  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
draw_data <- time_data %>%
  add_fitted_draws(model, seed = seed, re_formula = NA)

for (task_name in task_list) {
  draw_data_sub <- subset(draw_data, task == task_name)

  plot <- posterior_draws_plot(draw_data_sub, "dataset", FALSE, "Predicted Mean Completion Time (seconds)", "Oracle/Search Combination")
  plot
  filename = gsub("^.*\\.","", task_name )
  filename = gsub(" ", "_", filename)
  filename = paste("time", filename, sep = "")

  ggsave(
    file = paste(filename, ".png", sep = ""),
    plot = plot,
    path = "../plots/posterior_draws/time", width = 7, height = 7
  )
  
  fit_info <- draw_data_sub %>% group_by(search, oracle, dataset) %>% mean_qi(.value, .width = c(.95, .5))
  fit_info
  write.csv(fit_info,
            paste("../plot_data/posterior_draws/time/", filename, ".csv", sep = ""),
            row.names = FALSE)
}
```


```{r}
plot_data <- draw_data
plot_data <- plot_data[plot_data$task %in% c("1. Find Extremum", "2. Retrieve Value"),]
plot_data$task <- factor(plot_data$task)
plot_data$oracle<- gsub('compassql', 'CompassQL', plot_data$oracle)
plot_data$oracle<- gsub('dziban', 'Dziban', plot_data$oracle)
plot_data$search<- gsub('bfs', 'BFS', plot_data$search)
plot_data$search<- gsub('dfs', 'DFS', plot_data$search)
plot_data$dataset<- gsub('birdstrikes', 'Birdstrikes', plot_data$dataset)
plot_data$dataset<- gsub('movies', 'Movies', plot_data$dataset)
plot_data$Dataset<- plot_data$dataset

plot_data$condition <- paste(plot_data$oracle, plot_data$search, sep='\n')
draw_plot <- posterior_draws_plot(plot_data, "Dataset", TRUE, "Predicted Mean Completion Time (seconds)", "Oracle/Search Combination") + theme(axis.text.y=element_text(size=12) )+ scale_alpha(guide = 'none')  + xlab("Predicted Mean Completion Time (seconds)")

draw_plot
```

``` {r echo = FALSE}
# save the outputted plots and files
save_plot(draw_plot, "plot.png", "../plots/posterior_draws/time")

fit_info <-  plot_data %>% group_by(search, oracle, dataset, task) %>% mean_qi(.value, .width = c(.95, .5))
fit_info

write.csv(fit_info,
            paste("../plot_data/posterior_draws/time/fit.csv", sep = ""),
            row.names = FALSE)
```

## Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

```{r}
predictive_data <- time_data %>%
  add_fitted_draws(model, seed = seed, re_formula = NA)
```

### Differences Between Search
```{r}
search_differences <- expected_diff_in_mean_plot(predictive_data, "search", "Difference in Mean Completion Time (Seconds)",  "Task", NULL)

ggsave(file="search_time_differences.png", plot=search_differences$plot, path = "../plots/comparisons/time", width = 7, height = 7)

search_differences$plot

```

We can double-check the boundaries of the credible intervals to be sure whether or not the interval contains zero.
```{r}
write.csv(search_differences$intervals, "../plot_data/comparisons/time/search_time_differences.csv", sep="", row.names = FALSE)

search_differences$intervals
```

Let's do the above, but split it by datasets
```{r}
search_differences_split_by_dataset <- expected_diff_in_mean_plot(predictive_data, "search", "Difference in Mean Completion Time (Seconds)",  "Task", "dataset")

ggsave(file="split_by_dataset_search_time_differences.png", plot=search_differences_split_by_dataset$plot, path = "../plots/comparisons/time", width = 7, height = 7)

search_differences_split_by_dataset$plot
```


Check intervals
```{r}
write.csv(search_differences_split_by_dataset$intervals, "../plot_data/comparisons/time/search_time_differences_split_by_dataset.csv", row.names = FALSE)

search_differences_split_by_dataset$intervals

```

### Differences Between oracle
```{r}
oracle_differences <- expected_diff_in_mean_plot(predictive_data, "oracle", "Difference in Mean Completion Time (Seconds)",  "Task", NULL)

ggsave(file="oracle_time_differences.png", plot=oracle_differences$plot, path = "../plots/comparisons/time", width = 7, height = 7)

oracle_differences$plot

```


We can double-check the boundaries of the credible intervals to be sure whether or not the interval contains zero.
```{r}
write.csv(oracle_differences$intervals, "../plot_data/comparisons/time/oracle_time_differences.csv", sep="", row.names = FALSE)

oracle_differences$intervals
```

Let's do the above, but split it by datasets
```{r}
oracle_differences_subset_split_by_dataset <- expected_diff_in_mean_plot(predictive_data, "oracle", "Difference in Mean Completion Time (Seconds)",  "Task", "dataset")
ggsave(file="split_by_dataset_oracle_time_differences.png", plot=oracle_differences_subset_split_by_dataset$plot, path = "../plots/comparisons/time", width = 7, height = 7)

oracle_differences_subset_split_by_dataset$plot
```

Check intervals
```{r}
write.csv(oracle_differences_subset_split_by_dataset$intervals, "../plot_data/comparisons/time/oracle_time_differences_split_by_dataset.csv", row.names = FALSE)
oracle_differences_subset_split_by_dataset$intervals

```

### Differences Between Participant Groups
```{r}
participant_group_differences <- expected_diff_in_mean_plot(predictive_data, "participant_group", "Difference in Mean Completion Time (Seconds)",  "Task", NULL)

ggsave(file="participant_group_time_differences.png", plot=participant_group_differences$plot, path = "../plots/comparisons/time", width = 7, height = 7)

participant_group_differences$plot

```
Let's do the above, but split it by datasets
```{r}
participant_group_differences_split_by_dataset <- expected_diff_in_mean_plot(predictive_data, "participant_group", "Difference in Mean Completion Time (Seconds)",  "Task", "dataset")
ggsave(file="split_by_dataset_participant_group_time_differences.png", plot=participant_group_differences_split_by_dataset$plot, path = "../plots/comparisons/time", width = 7, height = 7)

participant_group_differences_split_by_dataset$plot
```

We can double-check the boundaries of the credible intervals to be sure whether or not the interval contains zero.
```{r}
write.csv(participant_group_differences$intervals, "../plot_data/comparisons/time/participant_group_time_differences.csv", sep="", row.names = FALSE)

participant_group_differences$intervals
```

## Summary Plots
Plot all of the posterior draws on one plot
```{r}
plot <- draw_data %>% ggplot(aes(
    x = .value,
    y = task,
    fill = search,
    alpha = 0.5
  )) + stat_halfeye(.width = c(.95, .5)) +
    labs(x = "Time (Seconds)", y = "Task") +  facet_grid(. ~ dataset)
plot

```

``` {r echo = FALSE}
ggsave(
    file = paste("all_tasks_search.png", sep = ""),
    plot = plot,
    path = "../plots/posterior_draws/time"
  )
```

#Code for additional plots (mostly subsets)
```{r}
predictive_data_subset <- predictive_data[predictive_data$task %in% c("1. Find Extremum", "2. Retrieve Value"),]
predictive_data_subset$task <- factor(predictive_data_subset$task)

search_differences_subset <- expected_diff_in_mean_plot(predictive_data_subset, "search", "Expected Time Difference (Seconds)",  "Task", NULL)

ggsave(file="search_time_differences_subset.png", plot=search_differences_subset$plot, path = "../plots/comparisons/time", width = 7, height = 7)

search_differences_subset$plot
```

```{r}
diff_in_search_prediction_split_by_dataset_subset <- expected_diff_in_mean_plot(predictive_data_subset, "search", "Expected Time Difference (Seconds)",  "Task", "dataset")
ggsave(file="split_by_dataset_search_time_differences_subset.png", plot=diff_in_search_prediction_split_by_dataset_subset$plot, path = "../plots/comparisons/time", width = 7, height = 7)

diff_in_search_prediction_split_by_dataset_subset$plot
```

```{r}
oracle_differences_subset <- expected_diff_in_mean_plot(predictive_data_subset, "oracle", "Expected Time Difference (Seconds)",  "Task", NULL)

ggsave(file="oracle_time_differences_subset.png", plot=oracle_differences_subset$plot, path = "../plots/comparisons/time", width = 7, height = 7)

oracle_differences_subset$plot
```
```{r}
oracle_differences_subset_split_by_dataset_subset <- expected_diff_in_mean_plot(predictive_data_subset, "oracle", "Expected Time Difference (Seconds)",  "Task", "dataset")
ggsave(file="split_by_dataset_oracle_time_differences_subset.png", plot=oracle_differences_subset_split_by_dataset_subset$plot, path = "../plots/comparisons/time", width = 7, height = 7)

oracle_differences_subset_split_by_dataset_subset$plot
```
