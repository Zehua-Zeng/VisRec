---
title: "task_response_analysis"
output: html_document
---

```{r libraries, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(fig.align="center") 
library(rstanarm)
library(tidyverse)
library(tidybayes)
library(modelr) 
library(ggplot2)
library(magrittr)  
library(emmeans)
library(bayesplot)

theme_set(theme_light())

```


###Read in data

```{r data_prep}

mydata = read.csv('processed_ptask_responses.csv')
time_data = read.csv('processed_completion_time.csv')
accuracy_data = read.csv('processed_accuracy_split.csv')

analyses = c("confidence.udata", "confidence.ans", "efficiency", "ease.of.use", "utility", "overall")
seed = 12
```

```{r posterior_plotting_function}
generate_posterior_plot = function(model, num_independent_variables, filename, x_lab, facet=TRUE, log_transformed=FALSE) {
  f = model %>%
    emmeans( ~ x1*x2*x3) %>%
    gather_emmeans_draws()
  
  # save the credible interval boundaries.  We will use the 90% credible interval.
  fit_info = f %>%
    mean_qi(.value, .width = c(.95, .5))
  
  # if facet is true, we split the data into two plots: one for the birdstrikes dataset, one for the movies dataset
  # the condition column will be used to label the y axis
  if (facet){
    f$condition = paste(f$x2, f$x3, sep= " | ")
  }
  else{
    f$condition = paste(f$x1, f$x2, f$x3, sep= " | ")
  }
  
  # create, format, and save our posterior plot
  plot = ggplot(f, aes(x = if(log_transformed) exp(.value) else .value, y = reorder(condition, desc(condition)), fill = x3)) + stat_halfeye(.width = c(.95, .5)) + 
    labs(x=x_lab, y="Condition")
  if(facet){
    plot = plot + facet_grid(cols = vars(x1))
      ggsave(file=paste(filename, ".png", sep=""), plot=plot, path = "../plots/posterior_plots", width = 6, height = 4, dpi = 300, units = "in")
  }
  else{
      ggsave(file=paste(filename, ".png", sep=""), plot=plot, path = "../plots/posterior_plots")
  }

  # save out the credible interval boundaries as well to use for the paper
  write.csv(fit_info, paste("../plot_data/",filename,".csv", sep=""), row.names = FALSE)
}
```

```{r task_response_ordinal}
library(tidyverse)
library(brms)
library(tidybayes)
library(gganimate)
data <- read.csv('processed_ptask_responses.csv')
response_levels =  c(-2,-1, 0, 1, 2)
response_labels = c("-2", "-1", "0", "1", "2")
data <- data %>%
  mutate(
    dataset = as.factor(dataset),
    oracle = as.factor(oracle),
    search = as.factor(search),
    task = as.factor(task),
    confidence.udata = ordered(
      confidence.udata,
      levels = c(-1, 0, 1, 2),
      labels = c("-1", "0", "1", "2")
    ),
    confidence.ans = ordered(
      confidence.ans,
      levels = response_levels,
      labels = response_labels
    ),
    efficiency = ordered(
      efficiency,
      levels = response_levels,
      labels = response_labels
    ),
    ease.of.use = ordered(
      ease.of.use,
      levels = response_levels,
      labels = response_labels
    ),
    utility = ordered(
      utility,
      levels = response_levels,
      labels = response_labels
    ),
    overall = ordered(
      overall,
      levels = response_levels,
      labels = response_labels
    ),
  )
model <- brm(
    formula = bf(overall ~ dataset * oracle * search * task),
    family = cumulative("probit"),
    prior = prior(normal(0, 1), class = b),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = data,
    control = list(adapt_delta = 0.99),
    file = "overall"
  )

generate_model_and_draws = function(data, attr, generate_eval_plots = FALSE) {
  filename = gsub("\\.", "_", attr)
  data$y = data[,attr]
  model <- brm(
    formula = bf(y ~ dataset * oracle * search * task),
    family = cumulative("probit"),
    prior = prior(normal(0, 1), class = b),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = data,
    control = list(adapt_delta = 0.99),
    file = filename
  )
  if(generate_eval_plots){
    summary(model)
  plot(model)
  
  pairs(
    model,
    pars = c(
      "b_Intercept",
      "b_datasetmovies",
      "b_oracledziban",
      "b_searchdfs",
      "b_datasetmovies:oracledziban",
      "b_datasetmovies:searchdfs",
      "b_oracledziban:searchdfs",
      "b_datasetmovies:oracledziban:searchdfs"
    ),
    fixed = TRUE
  )
  
  data %>%
    mutate(rating = ordered(y, levels = response_labels)) %>%
    ggplot(aes(x = oracle, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ search)
  
  data %>%
    add_predicted_draws(model, seed = seed) %>%
    group_by(search, oracle, dataset) %>%
    summarize(rating = .prediction) %>%
    ggplot(aes(x = oracle, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ search)
  
  spec <- data %>%
    add_predicted_draws(model, seed = seed, n = 50) %>%
    mutate(rating = .prediction) %>%
    ggplot(aes(x = oracle, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ search) +
    transition_manual(.draw)
  
    animate(spec, fps = 2.5)
  }
  
  predictive_plot <- data %>%
    add_predicted_draws(model, seed = seed, re_formula = NA) %>%
    group_by(search, oracle, .draw) %>%
    mutate(rating = weighted.mean(as.numeric(as.character(.prediction)))) %>%
    ggplot(aes(x = oracle, y = rating)) +
    stat_eye(.width = c(.95, .5)) +
    theme_minimal() +
    coord_cartesian(ylim = c(-2, 2)) +
    facet_grid(. ~ search)
  
  ggsave(file=paste(filename, ".png", sep=""), plot=predictive_plot)
  
  return(model)
}

# draws1 = generate_model_and_draws(data, "overall")
# draws2 = generate_model_and_draws(data, "confidence.ans")
# draws3 = generate_model_and_draws(data, "efficiency")
# draws4 = generate_model_and_draws(data, "ease.of.use")
# draws5 = generate_model_and_draws(data, "utility")
# draws6 = generate_model_and_draws(data, "confidence.udata")
# a <- rbind(draws1, draws2) %>% rbind(draws3)  %>% rbind(draws4)  %>% rbind(draws5)  %>% rbind(draws6)
search_draw_data_combined <- data.frame(draw=numeric(),
                 search=character(),
                 diff_in_rating=numeric(),
                 task=character(), 
                 stringsAsFactors=FALSE) 

oracle_draw_data_combined <- data.frame(draw=numeric(),
                 oracle=character(),
                 diff_in_rating=numeric(),
                 task=character(), 
                 stringsAsFactors=FALSE) 

for (attr in analyses) {
  model = generate_model_and_draws(data, attr)
  draw_data <- data %>%
    add_predicted_draws(model, seed = seed, re_formula = NA) 
  
  search_draw_data <- draw_data %>%
    group_by(search, .draw) %>%
    summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
    compare_levels(rating, by = search) %>%
    rename(diff_in_rating = rating) 
  
  search_draw_data['task'] = attr
  search_draw_data_combined <- rbind(search_draw_data_combined, search_draw_data)
  
  oracle_draw_data <- draw_data %>%
    group_by(oracle, .draw) %>%
    summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
    compare_levels(rating, by = oracle) %>%
    rename(diff_in_rating = rating) 
  
  oracle_draw_data['task'] = attr
  oracle_draw_data_combined <- rbind(oracle_draw_data_combined, oracle_draw_data)
}

search_differences_plot <-  search_draw_data_combined %>%
    ggplot(aes(x = diff_in_rating, y = task)) +
    ylab("Task") +
    xlab("Expected Difference in Rating (dfs - bfs)") +
    stat_halfeye() +
    geom_vline(xintercept = 0, linetype = "longdash") +
    theme_minimal()

ggsave(file="search_rating_differences.png", plot=search_differences_plot)

oracle_differences_plot <-  oracle_draw_data_combined %>%
    ggplot(aes(x = diff_in_rating, y = task)) +
    ylab("Task") +
    xlab("Expected Difference in Rating (dziban - compassql)") +
    stat_halfeye() +
    geom_vline(xintercept = 0, linetype = "longdash") +
    theme_minimal()

ggsave(file="oracle_rating_differences.png", plot=oracle_differences_plot)

```

```{r task_response_analysis}
# analysis to predict user score (ranges from -2 to 2) and different tasks given an oracle algorithm, search algorithm, and dataset
run_model_3_var = function(data, attr, split, response, a_prior, a_sd, x_lab){
  
  # if split, we want to split oracle and search when training the model rather than combining them into one field
  if(split){
    data$x1 = as.factor(data[,response])
    data$x2 = as.factor(data$oracle) 
    data$x3 = as.factor(data$search)
    data$y = data[,attr]
    
  }
  else{
    data$x1 = as.factor(data[,response])
    data$x2 = as.factor(data$dataset)
    data$x3 = as.factor(data$condition)
    data$y = data[,attr]
  }
  
  b1_prior = 0
  b1_sd = a_sd 
  
  # train model
  m = stan_glm(y ~ x1*x2*x3, data = data,
  prior_intercept = normal(a_prior, a_sd, autoscale = FALSE),
  prior = normal(b1_prior, b1_sd, autoscale = FALSE),
  seed = seed)
  
  filename = gsub("\\.", "_", attr)
  if(split){
    filename = paste(filename, "_split", sep="")
  }
  
  generate_posterior_plot(m, 3, filename, x_lab, FALSE)

}

for (attr in analyses){
  # weakly-informed prior on user response is N(0,1)
    run_model_3_var(mydata, attr, FALSE, "task", 0, 1, "User Score")
    run_model_3_var(mydata, attr, TRUE, "task", 0, 1, "User Score")
}

```

```{r time_analysis}  
# analysis to predict time elapsed for the user to complete a certain task given a dataset, oracle, and search algorithm

run_model_time = function(data, task, a_prior, a_sd, facet){
  #select your independent and dependent variables
  data$x1 = as.factor(data$dataset) 
  data$x2 = as.factor(data$oracle)
  data$x3 = as.factor(data$search)
  data$y = log(data$completion_time)
  
  b1_prior = 0
  b1_sd = a_sd 
  
  # train model
  m = stan_glm(y ~ x1*x2*x3, data = data,
  prior_intercept = normal(a_prior, a_sd, autoscale = FALSE),
  prior = normal(b1_prior, b1_sd, autoscale = FALSE),
  seed = seed)
  
  
  filename = gsub(" ", "_", task)
  filename = paste("time_", filename, sep="")
  generate_posterior_plot(m, 3, filename, "Time (Seconds)", facet, TRUE)

}

prior_mean = 360.48
prior_sd = 224.40

# priors for completion time of each task were derived from a pilot study
run_model_time(subset(time_data, task == "1. Find Extremum"), "Find Extremum", prior_mean, prior_sd, FALSE)
run_model_time(subset(time_data, task == "2. Retrieve Value"), "Retrieve Value", prior_mean, prior_sd, FALSE)
run_model_time(subset(time_data, task == "3. Prediction"), "Prediction", prior_mean, prior_sd, FALSE)
run_model_time(subset(time_data, task == "4. Exploration"), "Exploration", prior_mean, prior_sd, FALSE)

```

```{r analysis_focused_or_open}  
# The different tasks can be decribed as "focused" or "open-ended", so let's run analyses where we split on that attribute
run_model_time(subset(time_data, task_type == "Focused"), "Focused", prior_mean, prior_sd, TRUE)
run_model_time(subset(time_data, task_type == "Open-Ended"), "Open-Ended", prior_mean, prior_sd, TRUE)

for (attr in analyses){
    run_model_3_var(mydata, attr, TRUE, "task_type", 0, 1, "User Score")
}
```

```{r misc_analyses}
run_model_time(subset(time_data, task == "3. Prediction"), "Prediction (split by dataset)", 870.53, 657.68, TRUE)
run_model_time(subset(time_data, task == "1. Find Extremum"), "Find Extremum (split by dataset)", 190.34, 96.57, TRUE)
```


```{r accuracy_analysis}  
# analyses to predict the probability of getting a correct answer given a certain task, oracle, and search algorithm
accuracy_tasks = c("1. Find Extremum", "2. Retrieve Value")

run_model_accuracy = function(data, task){
  task_accuracy = subset(data, task == task)
  
  task_accuracy$oracle = as.factor(task_accuracy$oracle)
  task_accuracy$search = as.factor(task_accuracy$search)
  task_accuracy$dataset = as.factor(task_accuracy$dataset)
  task_accuracy$y = task_accuracy$accuracy
  
  task_accuracy$condition <- paste(task_accuracy$oracle, " | ", task_accuracy$search)
  
  # uninformed prior on coefficients?
  t_prior <- student_t(df = 2, location = 0, scale = 2.5)
  
  # train model
  m = stan_glm(y ~ oracle*search*dataset, data = task_accuracy, family = binomial(link = "logit"),
  prior_intercept = t_prior,
  prior = t_prior,
  seed = seed)
  
  print(posterior_interval(
  m,
  prob = 0.9))
  pplot <- plot(m, "areas", prob = 0.95, prob_outer = 1)
  pplot + geom_vline(xintercept = 0)
  pplot
  
  yrep = posterior_predict(m, draws = 1000)
  plot = ppc_violin_grouped(y = m$y, 
                     yrep = yrep, 
                     group = task_accuracy$condition, 
                     probs = c(0.05, 0.5, 0.95),
                     y_draw = "points", y_alpha = 0) +
    xlab("Condition") +
    ylab("Probability of a Correct Answer") + 
    legend_none() +
    scale_y_continuous(breaks=seq(0,1,.1))
  
  filename = gsub("\\.", "", task)
  filename = gsub(" ", "_", filename)
  ggsave(file=paste("accuracy_",filename, ".png", sep=""), plot=plot, path = "../plots/posterior_plots")
}

for (task in accuracy_tasks){
    run_model_accuracy(accuracy_data, task)
}

# TODO:  Figure out how to get the values of the credible interval boundaries that are shown on the plots

run_model_accuracy(accuracy_data, "1. Find Extremum")
  
```








