---
title: "task_response_analysis"
output: html_document
---

```{r libraries, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(fig.align="center") 
library(rstanarm)
library(tidyverse)
library(tidybayes)
library(modelr) 
library(ggplot2)
library(magrittr)  
library(emmeans)
library(bayesplot)
library(brms)
library(gganimate)

theme_set(theme_light())

```


###Read in data

```{r data_prep}

mydata = read.csv('processed_ptask_responses.csv')
time_data = read.csv('processed_completion_time.csv')
accuracy_data = read.csv('processed_accuracy_split.csv')

analyses = c("confidence.udata", "confidence.ans", "efficiency", "ease.of.use", "utility", "overall")
confidence_metrics = c("confidence.udata", "confidence.ans")
preference_metrics = c("efficiency", "ease.of.use", "utility", "overall")
seed = 12
```

```{r posterior_plotting_function}
generate_posterior_plot = function(model, num_independent_variables, filename, x_lab, facet=TRUE, log_transformed=FALSE) {
  f = model %>%
    emmeans( ~ x1*x2*x3) %>%
    gather_emmeans_draws()
  
  # save the credible interval boundaries.  We will use the 90% credible interval.
  fit_info = f %>%
    mean_qi(.value, .width = c(.95, .5))
  
  # if facet is true, we split the data into two plots: one for the birdstrikes dataset, one for the movies dataset
  # the condition column will be used to label the y axis
  if (facet){
    f$condition = paste(f$x2, f$x3, sep= " | ")
  }
  else{
    f$condition = paste(f$x1, f$x2, f$x3, sep= " | ")
  }
  
  # create, format, and save our posterior plot
  plot = ggplot(f, aes(x = if(log_transformed) exp(.value) else .value, y = reorder(condition, desc(condition)), fill = x3)) + stat_halfeye(.width = c(.95, .5)) + 
    labs(x=x_lab, y="Condition")
  if(facet){
    plot = plot + facet_grid(cols = vars(x1))
      ggsave(file=paste(filename, ".png", sep=""), plot=plot, path = "../plots/posterior_plots", width = 6, height = 4, dpi = 300, units = "in")
  }
  else{
      ggsave(file=paste(filename, ".png", sep=""), plot=plot, path = "../plots/posterior_plots")
  }

  # save out the credible interval boundaries as well to use for the paper
  write.csv(fit_info, paste("../plot_data/",filename,".csv", sep=""), row.names = FALSE)
}
```

```{r task_response_ordinal}
data <- read.csv('processed_ptask_responses.csv')
response_levels =  c(-2,-1, 0, 1, 2)
response_labels = c("-2", "-1", "0", "1", "2")
data <- data %>%
  mutate(
    dataset = as.factor(dataset),
    oracle = as.factor(oracle),
    search = as.factor(search),
    task = as.factor(task),
    confidence.udata = ordered(
      confidence.udata,
      levels = c(-1, 0, 1, 2),
      labels = c("-1", "0", "1", "2")
    ),
    confidence.ans = ordered(
      confidence.ans,
      levels = response_levels,
      labels = response_labels
    ),
    efficiency = ordered(
      efficiency,
      levels = response_levels,
      labels = response_labels
    ),
    ease.of.use = ordered(
      ease.of.use,
      levels = response_levels,
      labels = response_labels
    ),
    utility = ordered(
      utility,
      levels = response_levels,
      labels = response_labels
    ),
    overall = ordered(
      overall,
      levels = response_levels,
      labels = response_labels
    ),
  )

generate_model_and_draws = function(data, attr, generate_eval_plots = FALSE) {
  filename = gsub("\\.", "_", attr)
  data$y = data[,attr]
  model <- brm(
    formula = bf(y ~ dataset * oracle * search * task_type),
    family = cumulative("probit"),
    prior = prior(normal(0, 1), class = b),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = data,
    control = list(adapt_delta = 0.99),
    file = filename,
    seed = seed
  )
  if(generate_eval_plots){
    summary(model)
  plot(model)
  
  pairs(
    model,
    pars = c(
      "b_Intercept",
      "b_datasetmovies",
      "b_oracledziban",
      "b_searchdfs",
      "b_datasetmovies:oracledziban",
      "b_datasetmovies:searchdfs",
      "b_oracledziban:searchdfs",
      "b_datasetmovies:oracledziban:searchdfs"
    ),
    fixed = TRUE
  )
  
  data %>%
    mutate(rating = ordered(y, levels = response_labels)) %>%
    ggplot(aes(x = oracle, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ search)
  
  data %>%
    add_predicted_draws(model, seed = seed) %>%
    group_by(search, oracle, dataset) %>%
    summarize(rating = .prediction) %>%
    ggplot(aes(x = oracle, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ search)
  
  spec <- data %>%
    add_predicted_draws(model, seed = seed, n = 50) %>%
    mutate(rating = .prediction) %>%
    ggplot(aes(x = oracle, fill = rating)) +
    geom_bar() +
    theme_minimal() +
    facet_grid(. ~ search) +
    transition_manual(.draw)
  
    animate(spec, fps = 2.5)
  }
  
  predictive_plot_data <- data %>%
    add_predicted_draws(model, seed = seed, re_formula = NA) %>%
    group_by(search, oracle, .draw) %>%
    mutate(rating = weighted.mean(as.numeric(as.character(.prediction))))
  
  predictive_plot <- predictive_plot_data %>%
    ggplot(aes(x = oracle, y = rating)) +
    stat_eye(.width = c(.95, .5)) +
    theme_minimal() +
    coord_cartesian(ylim = c(-2, 2)) +
    facet_grid(. ~ search)
  
  ggsave(file=paste(filename, ".png", sep=""), plot=predictive_plot, path = "../plots/posterior_plots/new", width = 7, height = 7)
  credible_intervals <- predictive_plot_data %>% group_by(search, oracle) %>% mean_qi(rating, .width = c(.95, .5))
  write.csv(credible_intervals, paste("../plot_data/new/",filename,".csv", sep=""), row.names = FALSE, append=FALSE)
  
  # TODO:  Make sure alpha looks nice
  predictive_plot_focused_open_split <- predictive_plot_data %>%
    ggplot(aes(x = oracle, y = rating, fill=search, alpha = 0.5)) +
    stat_eye(.width = c(.95, .5)) +
    theme_minimal() +
    coord_cartesian(ylim = c(-2, 2)) +
    facet_grid(. ~ task_type)
  
  ggsave(file=paste(filename, "_focused_open_split.png", sep=""), plot=predictive_plot_focused_open_split, path = "../plots/posterior_plots/new", width = 7, height = 7)
  credible_intervals <- predictive_plot_data %>% group_by(search, oracle, task_type) %>% mean_qi(rating, .width = c(.95, .5))
  write.csv(credible_intervals, paste("../plot_data/new/",filename,"_focused_open_split.csv", sep=""), row.names = FALSE, append=FALSE)
  
  return(model)
}

search_draw_data_combined <- data.frame(draw=numeric(),
                 search=character(),
                 diff_in_rating=numeric(),
                 task=character(), 
                 stringsAsFactors=FALSE) 

search_differences_intervals <- data.frame(
                .lower=numeric(),
                .upper=numeric(),
                .width=numeric(),
                .point=character(),
                interval=character(),
                 search=character(),
                 diff_in_rating=numeric(),
                 task=character(), 
                 stringsAsFactors=FALSE) 

oracle_draw_data_combined <- data.frame(draw=numeric(),
                 oracle=character(),
                 diff_in_rating=numeric(),
                 task=character(), 
                 stringsAsFactors=FALSE) 

oracle_differences_intervals <- data.frame(
                .lower=numeric(),
                .upper=numeric(),
                .width=numeric(),
                .point=character(),
                interval=character(),
                 oracle=character(),
                 diff_in_rating=numeric(),
                 task=character(), 
                 stringsAsFactors=FALSE) 

alg_draw_data_combined <- data.frame(draw=numeric(),
                 alg=character(),
                 diff_in_rating=numeric(),
                 task=character(), 
                 stringsAsFactors=FALSE) 

alg_differences_intervals <- data.frame(
                .lower=numeric(),
                .upper=numeric(),
                .width=numeric(),
                .point=character(),
                interval=character(),
                 alg=character(),
                 diff_in_rating=numeric(),
                 task=character(), 
                 stringsAsFactors=FALSE) 


for (attr in analyses) {
  model = generate_model_and_draws(data, attr)
  draw_data <- data %>%
    add_predicted_draws(model, seed = seed, re_formula = NA) 
  
  search_draw_data <- draw_data %>%
    group_by(search, .draw) %>%
    summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
    compare_levels(rating, by = search) %>%
    rename(diff_in_rating = rating) 
  
  search_draw_data['metric'] = attr
  search_draw_data_combined <- rbind(search_draw_data_combined, search_draw_data)
  
  search_credible_intervals <- search_draw_data %>% mean_qi(diff_in_rating, .width = c(.95, .5))
  search_credible_intervals['metric'] = attr
  search_differences_intervals <- rbind(search_differences_intervals, search_credible_intervals)
  
  oracle_draw_data <- draw_data %>%
    group_by(oracle, .draw) %>%
    summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
    compare_levels(rating, by = oracle) %>%
    rename(diff_in_rating = rating) 
  
  oracle_draw_data['metric'] = attr
  oracle_draw_data_combined <- rbind(oracle_draw_data_combined, oracle_draw_data)
  
  oracle_credible_intervals <- oracle_draw_data %>% mean_qi(diff_in_rating, .width = c(.95, .5))
  oracle_credible_intervals['metric'] = attr
  oracle_differences_intervals <- rbind(oracle_differences_intervals, oracle_credible_intervals)
  
  alg_draw_data <- draw_data 
  alg_draw_data$alg <- paste(alg_draw_data$search, alg_draw_data$oracle)
  alg_draw_data$alg <- as.factor(alg_draw_data$alg)
   
  alg_draw_data <- alg_draw_data %>%
    group_by(alg, .draw) %>%
    summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
    compare_levels(rating, by = alg) %>%
    rename(diff_in_rating = rating)
  alg_draw_data['metric'] = attr
  alg_draw_data_combined <- rbind(alg_draw_data_combined, alg_draw_data)
  
  alg_credible_intervals <- alg_draw_data %>% mean_qi(diff_in_rating, .width = c(.95, .5))
  alg_credible_intervals['metric'] = attr
  alg_differences_intervals <- rbind(alg_differences_intervals, alg_credible_intervals)
}

# flip order so that we get bfs - dfs
if(search_draw_data_combined[1,'search']=="dfs - bfs"){
  search_draw_data_combined$search = 'bfs - dfs'
  search_draw_data_combined$diff_in_rating = -1 * search_draw_data_combined$diff_in_rating
}

if(search_differences_intervals[1,'search']=="dfs - bfs"){
  search_differences_intervals$search = 'bfs - dfs'
  search_differences_intervals$temp = -1 * search_differences_intervals$.lower
  search_differences_intervals$diff_in_rating = -1 * search_differences_intervals$diff_in_rating
  search_differences_intervals$lower = -1 * search_differences_intervals$.upper
  search_differences_intervals$.upper = search_differences_intervals$temp 
}
search_draw_data_combined$metric <- factor(search_draw_data_combined$metric, levels=rev(analyses))
search_differences_plot_confidence <-  subset(search_draw_data_combined, metric %in% confidence_metrics) %>%
    ggplot(aes(x = diff_in_rating, y = metric)) +
    ylab("Confidence") +
    xlab(paste0("Expected Difference in Rating (",search_draw_data_combined[1,'search'],")")) +
    stat_halfeye(.width = c(.95, .5)) +
    geom_vline(xintercept = 0, linetype = "longdash") +
    theme_minimal()

ggsave(file="search_rating_differences_confidence.png", plot=search_differences_plot_confidence, path = "../plots/posterior_plots/new", width = 7, height = 7)

search_differences_plot_preference <-  subset(search_draw_data_combined, metric %in% preference_metrics) %>%
    ggplot(aes(x = diff_in_rating, y = metric)) +
    ylab("Preference") +
    xlab(paste0("Expected Difference in Rating (",search_draw_data_combined[1,'search'],")")) +
    stat_halfeye(.width = c(.95, .5)) +
    geom_vline(xintercept = 0, linetype = "longdash") +
    theme_minimal()

ggsave(file="search_rating_differences_preference.png", plot=search_differences_plot_preference, path = "../plots/posterior_plots/new", width = 7, height = 7)

write.csv(search_differences_intervals, paste("../plot_data/new/search_rating_differences.csv", sep=""), row.names = FALSE)

oracle_draw_data_combined$metric <- factor(oracle_draw_data_combined$metric, levels=rev(analyses))

oracle_differences_plot_confidence <-  subset(oracle_draw_data_combined, metric %in% confidence_metrics) %>%
    ggplot(aes(x = diff_in_rating, y = metric)) +
    ylab("Confidence") +
    xlab(paste0("Expected Difference in Rating (",oracle_draw_data_combined[1,'oracle'],")")) +
    stat_halfeye(.width = c(.95, .5)) +
    geom_vline(xintercept = 0, linetype = "longdash") +
    theme_minimal()

ggsave(file="oracle_rating_differences_confidence.png", plot=oracle_differences_plot_confidence, path = "../plots/posterior_plots/new", width = 7, height = 7)

oracle_differences_plot_preference <-  subset(oracle_draw_data_combined, metric %in% preference_metrics) %>%
    ggplot(aes(x = diff_in_rating, y = metric)) +
    ylab("Preference") +
    xlab(paste0("Expected Difference in Rating (",oracle_draw_data_combined[1,'oracle'],")")) +
    stat_halfeye(.width = c(.95, .5)) +
    geom_vline(xintercept = 0, linetype = "longdash") +
    theme_minimal()

ggsave(file="oracle_rating_differences_preference.png", plot=oracle_differences_plot_preference, path = "../plots/posterior_plots/new", width = 7, height = 7)

write.csv(oracle_differences_intervals, paste("../plot_data/new/oracle_rating_differences.csv", sep=""), row.names = FALSE)


alg_draw_data_combined = subset(alg_draw_data_combined, alg == "dfs compassql - bfs dziban")
alg_draw_data_combined$metric <- factor(alg_draw_data_combined$metric, levels=rev(analyses))
alg_draw_data_combined$alg <- as.factor(alg_draw_data_combined$alg)
alg_differences_plot <- subset(alg_draw_data_combined, metric %in% preference_metrics) %>%
    ggplot(aes(x = diff_in_rating, y = metric)) +
    ylab("Preference") +
    xlab("Expected Difference in Rating (dfs compassql - bfs dziban)") +
    stat_halfeye(.width = c(.95, .5)) +
    geom_vline(xintercept = 0, linetype = "longdash") +
    theme_minimal()

ggsave(file="alg_rating_differences_preference.png", plot=alg_differences_plot, path = "../plots/posterior_plots/new", width = 7, height = 7)
write.csv(alg_differences_intervals, paste("../plot_data/new/alg_rating_differences.csv", sep=""), row.names = FALSE)



```



```{r time_analysis}  
# analysis to predict time elapsed for the user to complete a certain task given a dataset, oracle, and search algorithm

run_model_time = function(data, task, a_prior, a_sd, facet){
  #select your independent and dependent variables
  data$x1 = as.factor(data$dataset) 
  data$x2 = as.factor(data$oracle)
  data$x3 = as.factor(data$search)
  data$y = log(data$completion_time)
  
  b1_prior = 0
  b1_sd = a_sd 
  
  # train model
  m = stan_glm(y ~ x1*x2*x3, data = data,
  prior_intercept = normal(a_prior, a_sd, autoscale = FALSE),
  prior = normal(b1_prior, b1_sd, autoscale = FALSE),
  seed = seed)
  
  
  filename = gsub(" ", "_", task)
  filename = paste("time_", filename, sep="")
  generate_posterior_plot(m, 3, filename, "Time (Seconds)", facet, TRUE)

}

prior_mean = 360.48
prior_sd = 224.40

# priors for completion time of each task were derived from a pilot study
run_model_time(subset(time_data, task == "1. Find Extremum"), "Find Extremum", prior_mean, prior_sd, FALSE)
run_model_time(subset(time_data, task == "2. Retrieve Value"), "Retrieve Value", prior_mean, prior_sd, FALSE)
run_model_time(subset(time_data, task == "3. Prediction"), "Prediction", prior_mean, prior_sd, FALSE)
run_model_time(subset(time_data, task == "4. Exploration"), "Exploration", prior_mean, prior_sd, FALSE)

```

```{r time_analysis_focused_or_open}  
# The different tasks can be decribed as "focused" or "open-ended", so let's run analyses where we split on that attribute
run_model_time(subset(time_data, task_type == "Focused"), "Focused", prior_mean, prior_sd, TRUE)
run_model_time(subset(time_data, task_type == "Open-Ended"), "Open-Ended", prior_mean, prior_sd, TRUE)

for (attr in analyses){
    run_model_3_var(mydata, attr, TRUE, "task_type", 0, 1, "User Score")
}
```

```{r misc_analyses}
run_model_time(subset(time_data, task == "3. Prediction"), "Prediction (split by dataset)", 870.53, 657.68, TRUE)
run_model_time(subset(time_data, task == "1. Find Extremum"), "Find Extremum (split by dataset)", 190.34, 96.57, TRUE)
```


```{r accuracy_analysis}  
# analyses to predict the probability of getting a correct answer given a certain task, oracle, and search algorithm
accuracy_tasks = c("1. Find Extremum", "2. Retrieve Value")

run_model_accuracy = function(data, task){
  task_accuracy = subset(data, task == task)
  
  task_accuracy$oracle = as.factor(task_accuracy$oracle)
  task_accuracy$search = as.factor(task_accuracy$search)
  task_accuracy$dataset = as.factor(task_accuracy$dataset)
  task_accuracy$y = task_accuracy$accuracy
  
  task_accuracy$condition <- paste(task_accuracy$oracle, " | ", task_accuracy$search)
  
  
  # train model
  m = stan_glm(y ~ oracle*search*dataset, data = task_accuracy, family = binomial(link = "logit"),
  prior_intercept = normal(.15, .06, autoscale = FALSE),
  prior = student_t(df = 2, location = 0, scale = 2.5),
  seed = seed)
  
  print(posterior_interval(
  m,
  prob = 0.9))
  pplot <- plot(m, "areas", prob = 0.95, prob_outer = 1)
  pplot + geom_vline(xintercept = 0)
  pplot
  
  yrep = posterior_predict(m, draws = 1000)
  print(yrep)
  plot = ppc_violin_grouped(y = m$y, 
                     yrep = yrep, 
                     group = task_accuracy$condition, 
                     probs = c(0.05, 0.5, 0.95),
                     y_draw = "points", y_alpha = 0) +
    xlab("Condition") +
    ylab("Probability of a Correct Answer") + 
    legend_none() +
    scale_y_continuous(breaks=seq(0,1,.1))
  
  filename = gsub("\\.", "", task)
  filename = gsub(" ", "_", filename)
  ggsave(file=paste("accuracy_",filename, ".png", sep=""), plot=plot, path = "../plots/posterior_plots")
  # credible_intervals <- yrep %>% mean_qi(diff_in_rating, .width = c(.95, .5))
}

for (task in accuracy_tasks){
    run_model_accuracy(accuracy_data, task)
}

# TODO:  Figure out how to get the values of the credible interval boundaries that are shown on the plots

run_model_accuracy(accuracy_data, "1. Find Extremum")
  
```

```{r accuracy_analysis_test}  
# analyses to predict the probability of getting a correct answer given a certain task, oracle, and search algorithm
accuracy_data = read.csv('processed_accuracy_split.csv')
accuracy_tasks = c("1. Find Extremum", "2. Retrieve Value")
accuracy_data$oracle = as.factor(accuracy_data$oracle)
accuracy_data$search = as.factor(accuracy_data$search)
accuracy_data$dataset = as.factor(accuracy_data$dataset)

run_model_accuracy = function(data, task){
  task_accuracy = subset(data, task == task)
  task_accuracy$y = task_accuracy$accuracy
  
  
  
  # train model
  
   m0 = stan_glm(y ~ oracle*search*dataset, data = task_accuracy, family = binomial(link = "logit"),
  prior_intercept = normal(.15, .06, autoscale = FALSE),
  prior = student_t(df = 2, location = 0, scale = 2.5),
  seed = seed)
   
  
    pplot <- plot(m0, "areas", prob = 0.95, prob_outer = 1)
    pplot + geom_vline(xintercept = 0)
    pplot
    
    result <- task_accuracy %>%
    group_by(oracle, search, dataset) %>%
    add_fitted_draws(m0, re_formula = NA, scale = "linear") 
    
    result %>%
    ggplot(aes(x = oracle, y = .value)) +
    coord_cartesian(ylim = c(0, 1)) +
    stat_eye(.width = c(.95, .5)) +
    theme_minimal() +
    facet_grid(. ~ search)
     
    yrep = posterior_predict(m, draws = 1000)
    print(yrep)
    plot = ppc_violin_grouped(y = m0$y, 
                       yrep = yrep, 
                       group = task_accuracy$oracle, 
                       probs = c(0.05, 0.5, 0.95),
                       y_draw = "points", y_alpha = 0) +
      xlab("Condition") +
      ylab("Probability of a Correct Answer") + 
      legend_none() +
      scale_y_continuous(breaks=seq(0,1,.1))
   
    
   
   
   
   
   
   
   
   
   m <- brm(
    data = task_accuracy, family = bernoulli(link = "logit"),
    formula = bf(y ~ oracle*search*dataset),
    prior = c(prior(normal(.15, 06), class = Intercept),
            prior(normal(0, 0.5), class = b)),
    iter = 8000, warmup = 2000, chains = 2, cores = 2, thin = 2,
    seed = seed)
   
   # get accuracy per condition from linear model
  result <- task_accuracy %>%
    group_by(oracle, search, dataset) %>%
    add_fitted_draws(m0, re_formula = NA, scale = "linear") %>%
    rename(p_correct = .value)
  
   predictive_plot_data <- data %>%
    add_predicted_draws(model, seed = seed, re_formula = NA) %>%
    group_by(search, oracle, .draw) %>%
    mutate(rating = weighted.mean(as.numeric(as.character(.prediction))))
  
  predictive_plot0 <- result %>%
    ggplot(aes(x = oracle, y = p_correct)) +
    stat_eye(.width = c(.95, .5)) +
    theme_minimal() +
    facet_grid(. ~ search)
    
  print(posterior_interval(
  m,
  prob = 0.9))
  pplot <- plot(m, "areas", prob = 0.95, prob_outer = 1)
  pplot + geom_vline(xintercept = 0)
  pplot
  
  yrep = posterior_predict(m, draws = 1000)
  print(yrep)
  plot = ppc_violin_grouped(y = m$y, 
                     yrep = yrep, 
                     group = task_accuracy$condition, 
                     probs = c(0.05, 0.5, 0.95),
                     y_draw = "points", y_alpha = 0) +
    xlab("Condition") +
    ylab("Probability of a Correct Answer") + 
    legend_none() +
    scale_y_continuous(breaks=seq(0,1,.1))
  
  filename = gsub("\\.", "", task)
  filename = gsub(" ", "_", filename)
  ggsave(file=paste("accuracy_",filename, ".png", sep=""), plot=plot, path = "../plots/posterior_plots/new")
  # credible_intervals <- yrep %>% mean_qi(diff_in_rating, .width = c(.95, .5))
}

for (task in accuracy_tasks){
    run_model_accuracy(accuracy_data, task)
}

# TODO:  Figure out how to get the values of the credible interval boundaries that are shown on the plots

run_model_accuracy(accuracy_data, "1. Find Extremum")
  
```







