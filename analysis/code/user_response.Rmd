---
title: "user_response"
output: html_document
---


### Libraries required for this analysis
```{r libraries, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(fig.align="center") 
library(rstanarm)
library(tidyverse)
library(tidybayes)
library(modelr) 
library(ggplot2)
library(magrittr)  
library(emmeans)
library(bayesplot)
library(brms)
library(gganimate)

theme_set(theme_light())

```

In our experiment, we used a visualization recommendation algorithm (composed of one search algorithm and one oracle algorithm) to generate visualizations for the user on one of two datasets.  We then asked the user to evaluate the tool on a variety of metrics (confidence in understanding data, confidence in answer, efficiency, ease of use, utility, and overall).

Given a search algorithm (bfs or dfs), an oracle (compassql or dziban), and a dataset (birdstrikes or movies), we would like to predict a user's average score for a given metric.  In addition, we would like to know if the choice of search algorithm and oracle has any meaningful impact on a user's ratong for these metrics.

### Read in and clean data

```{r data_prep}
analyses = c("confidence.udata", "confidence.ans", "efficiency", "ease.of.use", "utility", "overall")
confidence_metrics = c("confidence.udata", "confidence.ans")
preference_metrics = c("efficiency", "ease.of.use", "utility", "overall")

user_response_data <- read.csv('processed_ptask_responses.csv')
analyses = c("confidence.udata", "confidence.ans", "efficiency", "ease.of.use", "utility", "overall")
user_response_data[,analyses] <- lapply(user_response_data[,analyses],ordered)
user_response_data <- user_response_data %>%
  mutate(
    dataset = as.factor(dataset),
    oracle = as.factor(oracle),
    search = as.factor(search),
    task = as.factor(task)
  )

models <- list()

search_differences <- list()
oracle_differences <- list()
alg_differences <- list()


seed = 12
```

## Analysis for user responses

### Confidence in Understanding Data:  Building a Model

```{r}
filename = "confidence_udata"

models$confidence_udata <- brm(
    formula = bf(confidence.udata ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
    prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = filename,
    seed = seed
  )

```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$confidence_udata)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$confidence_udata)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$confidence_udata,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$confidence_udata,
  pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for confidence in understanding the data using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
draw_data_confidence_udata <- user_response_data %>%
  add_predicted_draws(models$confidence_udata,
                   seed = seed,
                   re_formula = NA) %>%
  group_by(search, oracle, .draw) %>%
  mutate(rating = weighted.mean(as.numeric(as.character(.prediction))))
  
confidence_udata_plot <- draw_data_confidence_udata %>%
  ggplot(aes(x = oracle, y = rating)) +
  stat_eye(.width = c(.95, .5)) +
  theme_minimal() +
  coord_cartesian(ylim = c(-2, 2)) +
  facet_grid(. ~ search)

confidence_udata_plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
fit_info_confidence_udata <- draw_data_confidence_udata %>% group_by(search, oracle) %>% mean_qi(rating, .width = c(.95, .5))
fit_info_confidence_udata
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "confidence_udata.png", plot = confidence_udata_plot, path = "../plots/posterior_draws/user_response")
  write.csv(fit_info_confidence_udata, "../plot_data/posterior_draws/user_response/confidence_udata.csv", row.names = FALSE)
```

### Confidence in Understanding Data: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
confidence_udata_predictive_data <- user_response_data %>% add_predicted_draws(models$confidence_udata, seed = seed, re_formula = NA) 
confidence_udata_predictive_data$alg <- paste(confidence_udata_predictive_data$search, confidence_udata_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$confidence_udata <- confidence_udata_predictive_data %>% 
  group_by(search, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = search) %>%
   rename(diff_in_rating = rating)

search_differences$confidence_udata$metric = "confidence.udata"

search_differences$confidence_udata %>%
      ggplot(aes(x = diff_in_rating, y = "confidence.udata")) +
      xlab(paste0("Expected Difference in Rating (",search_differences$confidence_udata[1,'search'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```

Differences in user score by oracle.
``` {r}
oracle_differences$confidence_udata <- confidence_udata_predictive_data %>% 
  group_by(oracle, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = oracle) %>%
   rename(diff_in_rating = rating)

oracle_differences$confidence_udata$metric = "confidence.udata"

oracle_differences$confidence_udata %>%
      ggplot(aes(x = diff_in_rating, y = "confidence.udata")) +
      xlab(paste0("Expected Difference in Rating (",oracle_differences$confidence_udata[1,'oracle'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}

alg_differences$confidence_udata <- confidence_udata_predictive_data %>% 
  group_by(alg, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = alg) %>%
   rename(diff_in_rating = rating)

alg_differences$confidence_udata <- subset(alg_differences$confidence_udata, alg == "dfs compassql - bfs dziban")
alg_differences$confidence_udata$metric = "confidence.udata"

alg_differences$confidence_udata %>%
      ggplot(aes(x = diff_in_rating, y = "confidence.udata")) +
      xlab(paste0("Expected Difference in Rating (",alg_differences$confidence_udata[1,'alg'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```






### Confidence in Answer:  Building a Model

```{r}
filename = "confidence_ans"
models$confidence_ans <- brm(
    formula = bf(confidence.ans ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
    prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = filename,
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$confidence_ans)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$confidence_ans)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$confidence_ans,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$confidence_ans,
  pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for confidence in answer using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
draw_data_confidence_ans <- user_response_data %>%
  add_predicted_draws(models$confidence_ans,
                   seed = seed,
                   re_formula = NA) %>%
  group_by(search, oracle, .draw) %>%
  mutate(rating = weighted.mean(as.numeric(as.character(.prediction))))
  
confidence_ans_plot <- draw_data_confidence_ans %>%
  ggplot(aes(x = oracle, y = rating)) +
  stat_eye(.width = c(.95, .5)) +
  theme_minimal() +
  coord_cartesian(ylim = c(-2, 2)) +
  facet_grid(. ~ search)

confidence_ans_plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
fit_info_confidence_ans <- draw_data_confidence_ans %>% group_by(search, oracle) %>% mean_qi(rating, .width = c(.95, .5))
fit_info_confidence_ans
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "confidence_ans.png", plot = confidence_ans_plot, path = "../plots/posterior_draws/user_response")
  write.csv(fit_info_confidence_ans, "../plot_data/posterior_draws/user_response/confidence_ans.csv", row.names = FALSE)
```

### Confidence in Answer: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
confidence_ans_predictive_data <- user_response_data %>% add_predicted_draws(models$confidence_ans, seed = seed, re_formula = NA) 
confidence_ans_predictive_data$alg <- paste(confidence_ans_predictive_data$search, confidence_ans_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$confidence_ans <- confidence_ans_predictive_data %>% 
  group_by(search, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = search) %>%
   rename(diff_in_rating = rating)

search_differences$confidence_ans$metric = "confidence.ans"

search_differences$confidence_ans %>%
      ggplot(aes(x = diff_in_rating, y = "confidence.ans")) +
      xlab(paste0("Expected Difference in Rating (",search_differences$confidence_ans[1,'search'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```

Differences in user score by oracle.
``` {r}
oracle_differences$confidence_ans <- confidence_ans_predictive_data %>% 
  group_by(oracle, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = oracle) %>%
   rename(diff_in_rating = rating)

oracle_differences$confidence_ans$metric = "confidence.ans"

oracle_differences$confidence_ans %>%
      ggplot(aes(x = diff_in_rating, y = "confidence.ans")) +
      xlab(paste0("Expected Difference in Rating (",oracle_differences$confidence_ans[1,'oracle'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}

alg_differences$confidence_ans <- confidence_ans_predictive_data %>% 
  group_by(alg, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = alg) %>%
   rename(diff_in_rating = rating)

alg_differences$confidence_ans <- subset(alg_differences$confidence_ans, alg == "dfs compassql - bfs dziban")
alg_differences$confidence_ans$metric = "confidence.ans"

alg_differences$confidence_ans %>%
      ggplot(aes(x = diff_in_rating, y = "confidence.ans")) +
      xlab(paste0("Expected Difference in Rating (",alg_differences$confidence_ans[1,'alg'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```





### Efficiency:  Building a Model

```{r}
filename = "efficiency"
models$efficiency <- brm(
    formula = bf(efficiency ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = filename,
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$efficiency)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$efficiency)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$efficiency,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$efficiency,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for efficiency using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
draw_data_efficiency <- user_response_data %>%
  add_predicted_draws(models$efficiency,
                   seed = seed,
                   re_formula = NA) %>%
  group_by(search, oracle, .draw) %>%
  mutate(rating = weighted.mean(as.numeric(as.character(.prediction))))
  
efficiency_plot <- draw_data_efficiency %>%
  ggplot(aes(x = oracle, y = rating)) +
  stat_eye(.width = c(.95, .5)) +
  theme_minimal() +
  coord_cartesian(ylim = c(-2, 2)) +
  facet_grid(. ~ search)

efficiency_plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
fit_info_efficiency <- draw_data_efficiency %>% group_by(search, oracle) %>% mean_qi(rating, .width = c(.95, .5))
fit_info_efficiency
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "efficiency.png", plot = efficiency_plot, path = "../plots/posterior_draws/user_response")
  write.csv(fit_info_efficiency, "../plot_data/posterior_draws/user_response/efficiency.csv", row.names = FALSE)
```

### Efficiency: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
efficiency_predictive_data <- user_response_data %>% add_predicted_draws(models$efficiency, seed = seed, re_formula = NA) 
efficiency_predictive_data$alg <- paste(efficiency_predictive_data$search, efficiency_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$efficiency <- efficiency_predictive_data %>% 
  group_by(search, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = search) %>%
   rename(diff_in_rating = rating)

search_differences$efficiency$metric = "efficiency"

search_differences$efficiency %>%
      ggplot(aes(x = diff_in_rating, y = "efficiency")) +
      xlab(paste0("Expected Difference in Rating (",search_differences$efficiency[1,'search'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```

Differences in user score by oracle.
``` {r}
oracle_differences$efficiency <- efficiency_predictive_data %>% 
  group_by(oracle, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = oracle) %>%
   rename(diff_in_rating = rating)

oracle_differences$efficiency$metric = "efficiency"

oracle_differences$efficiency %>%
      ggplot(aes(x = diff_in_rating, y = "efficiency")) +
      xlab(paste0("Expected Difference in Rating (",oracle_differences$efficiency[1,'oracle'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}

alg_differences$efficiency <- efficiency_predictive_data %>% 
  group_by(alg, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = alg) %>%
   rename(diff_in_rating = rating)

alg_differences$efficiency <- subset(alg_differences$efficiency, alg == "dfs compassql - bfs dziban")
alg_differences$efficiency$metric = "efficiency"


alg_differences$efficiency %>%
      ggplot(aes(x = diff_in_rating, y = "efficiency")) +
      xlab(paste0("Expected Difference in Rating (",alg_differences$efficiency[1,'alg'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```





### Ease of Use:  Building a Model

```{r}
filename = "ease_of_use"
models$ease_of_use <- brm(
    formula = bf(ease.of.use ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = filename,
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$ease_of_use)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$ease_of_use)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$ease_of_use,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$ease_of_use,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for ease of use using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
draw_data_ease_of_use <- user_response_data %>%
  add_predicted_draws(models$ease_of_use,
                   seed = seed,
                   re_formula = NA) %>%
  group_by(search, oracle, .draw) %>%
  mutate(rating = weighted.mean(as.numeric(as.character(.prediction))))
  
ease_of_use_plot <- draw_data_ease_of_use %>%
  ggplot(aes(x = oracle, y = rating)) +
  stat_eye(.width = c(.95, .5)) +
  theme_minimal() +
  coord_cartesian(ylim = c(-2, 2)) +
  facet_grid(. ~ search)

ease_of_use_plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
fit_info_ease_of_use <- draw_data_ease_of_use %>% group_by(search, oracle) %>% mean_qi(rating, .width = c(.95, .5))
fit_info_ease_of_use
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "ease_of_use.png", plot = ease_of_use_plot, path = "../plots/posterior_draws/user_response")
  write.csv(fit_info_ease_of_use, "../plot_data/posterior_draws/user_response/ease_of_use.csv", row.names = FALSE)
```

### Ease of Use: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
ease_of_use_predictive_data <- user_response_data %>% add_predicted_draws(models$ease_of_use, seed = seed, re_formula = NA) 
ease_of_use_predictive_data$alg <- paste(ease_of_use_predictive_data$search, ease_of_use_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$ease_of_use <- ease_of_use_predictive_data %>% 
  group_by(search, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = search) %>%
   rename(diff_in_rating = rating)

search_differences$ease_of_use$metric = "ease.of.use"

search_differences$ease_of_use %>%
      ggplot(aes(x = diff_in_rating, y = "ease.of.use")) +
      xlab(paste0("Expected Difference in Rating (",search_differences$ease_of_use[1,'search'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```

Differences in user score by oracle.
``` {r}
oracle_differences$ease_of_use <- ease_of_use_predictive_data %>% 
  group_by(oracle, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = oracle) %>%
   rename(diff_in_rating = rating)

oracle_differences$ease_of_use$metric = "ease.of.use"

oracle_differences$ease_of_use %>%
      ggplot(aes(x = diff_in_rating, y = "ease.of.use")) +
      xlab(paste0("Expected Difference in Rating (",oracle_differences$ease_of_use[1,'oracle'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}

alg_differences$ease_of_use <- ease_of_use_predictive_data %>% 
  group_by(alg, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = alg) %>%
   rename(diff_in_rating = rating)

alg_differences$ease_of_use <- subset(alg_differences$ease_of_use, alg == "dfs compassql - bfs dziban")
alg_differences$ease_of_use$metric = "ease.of.use"

alg_differences$ease_of_use %>%
      ggplot(aes(x = diff_in_rating, y = "ease.of.use")) +
      xlab(paste0("Expected Difference in Rating (",alg_differences$ease_of_use[1,'alg'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```





### Utility:  Building a Model

```{r}
filename = "utility"
models$utility <- brm(
    formula = bf(utility ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = filename,
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$utility)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$utility)
```

s plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$utility,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$utility,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for Utility using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
draw_data_utility <- user_response_data %>%
  add_predicted_draws(models$utility,
                   seed = seed,
                   re_formula = NA) %>%
  group_by(search, oracle, .draw) %>%
  mutate(rating = weighted.mean(as.numeric(as.character(.prediction))))
  
utility_plot <- draw_data_utility %>%
  ggplot(aes(x = oracle, y = rating)) +
  stat_eye(.width = c(.95, .5)) +
  theme_minimal() +
  coord_cartesian(ylim = c(-2, 2)) +
  facet_grid(. ~ search)

utility_plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
fit_info_utility <- draw_data_utility %>% group_by(search, oracle) %>% mean_qi(rating, .width = c(.95, .5))
fit_info_utility
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "utility.png", plot = utility_plot, path = "../plots/posterior_draws/user_response")
  write.csv(fit_info_utility, "../plot_data/posterior_draws/user_response/utility.csv", row.names = FALSE)
```

### Utility: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
utility_predictive_data <- user_response_data %>% add_predicted_draws(models$utility, seed = seed, re_formula = NA) 
utility_predictive_data$alg <- paste(utility_predictive_data$search, utility_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$utility <- utility_predictive_data %>% 
  group_by(search, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = search) %>%
   rename(diff_in_rating = rating)

search_differences$utility$metric = "utility"

search_differences$utility %>%
      ggplot(aes(x = diff_in_rating, y = "utility")) +
      xlab(paste0("Expected Difference in Rating (",search_differences$utility[1,'search'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```

Differences in user score by oracle.
``` {r}
oracle_differences$utility <- utility_predictive_data %>% 
  group_by(oracle, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = oracle) %>%
   rename(diff_in_rating = rating)

oracle_differences$utility$metric = "utility"

oracle_differences$utility %>%
      ggplot(aes(x = diff_in_rating, y = "utility")) +
      xlab(paste0("Expected Difference in Rating (",oracle_differences$utility[1,'oracle'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}

alg_differences$utility <- utility_predictive_data %>% 
  group_by(alg, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = alg) %>%
   rename(diff_in_rating = rating)

alg_differences$utility <- subset(alg_differences$utility, alg == "dfs compassql - bfs dziban")
alg_differences$utility$metric = "utility"

alg_differences$utility %>%
      ggplot(aes(x = diff_in_rating, y = "utility")) +
      xlab(paste0("Expected Difference in Rating (",alg_differences$utility[1,'alg'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```





### Overall:  Building a Model

```{r}
filename = "overall"
models$overall <- brm(
    formula = bf(overall ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = filename,
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$overall)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$overall)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$overall,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$overall,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for Overall using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
draw_data_overall <- user_response_data %>%
  add_predicted_draws(models$overall,
                   seed = seed,
                   re_formula = NA) %>%
  group_by(search, oracle, .draw) %>%
  mutate(rating = weighted.mean(as.numeric(as.character(.prediction))))
  
overall_plot <- draw_data_overall %>%
  ggplot(aes(x = oracle, y = rating)) +
  stat_eye(.width = c(.95, .5)) +
  theme_minimal() +
  coord_cartesian(ylim = c(-2, 2)) +
  facet_grid(. ~ search)

overall_plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
fit_info_overall <- draw_data_overall %>% group_by(search, oracle) %>% mean_qi(rating, .width = c(.95, .5))
fit_info_overall
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "overall.png", plot = overall_plot, path = "../plots/posterior_draws/user_response")
  write.csv(fit_info_overall, "../plot_data/posterior_draws/user_response/overall.csv", row.names = FALSE)
```

### Overall: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
overall_predictive_data <- user_response_data %>% add_predicted_draws(models$overall, seed = seed, re_formula = NA) 
overall_predictive_data$alg <- paste(overall_predictive_data$search, overall_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$overall <- overall_predictive_data %>% 
  group_by(search, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = search) %>%
   rename(diff_in_rating = rating)

search_differences$overall$metric = "overall"

search_differences$overall %>%
      ggplot(aes(x = diff_in_rating, y = "overall")) +
      xlab(paste0("Expected Difference in Rating (",search_differences$overall[1,'search'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```

Differences in user score by oracle.
``` {r}
oracle_differences$overall <- overall_predictive_data %>% 
  group_by(oracle, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = oracle) %>%
   rename(diff_in_rating = rating)

oracle_differences$overall$metric = "overall"

oracle_differences$overall %>%
      ggplot(aes(x = diff_in_rating, y = "overall")) +
      xlab(paste0("Expected Difference in Rating (",oracle_differences$overall[1,'oracle'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}

alg_differences$overall <- overall_predictive_data %>% 
  group_by(alg, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = alg) %>%
   rename(diff_in_rating = rating)

alg_differences$overall <- subset(alg_differences$overall, alg == "dfs compassql - bfs dziban")
alg_differences$overall$metric = "overall"

alg_differences$overall %>%
      ggplot(aes(x = diff_in_rating, y = "overall")) +
      xlab(paste0("Expected Difference in Rating (",alg_differences$overall[1,'alg'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
```





## Summary Plots
Putting the all of the plots for search algorithm and oracle differences together, split by whether the rating metric is of type confidence or preference  We'll start with differences in search algorithms.

### Differences in Search Algorithms
```{r}
combined_search_differences <- rbind(search_differences$confidence_udata, search_differences$confidence_ans, search_differences$efficiency, search_differences$ease_of_use, search_differences$utility, search_differences$overall)

combined_search_differences$metric <- factor(combined_search_differences$metric, levels=rev(analyses))

# flip order so that we get bfs - dfs
if(combined_search_differences[1,'search']=="dfs - bfs"){
  combined_search_differences$search = 'bfs - dfs'
  combined_search_differences$diff_in_rating = -1 * combined_search_differences$diff_in_rating
}

combined_search_differences_confidence <- subset(combined_search_differences, metric %in% confidence_metrics)
search_differences_plot_confidence <- combined_search_differences_confidence %>%
      ggplot(aes(x = diff_in_rating, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_search_differences_confidence[1,'search'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()

search_differences_plot_confidence
```

View intervals
```{r}
fit_info_search_differences_confidence <- combined_search_differences_confidence %>% group_by(search, metric) %>% mean_qi(diff_in_rating, .width = c(.95, .5))
fit_info_search_differences_confidence
```

```{r}
combined_search_differences_preference <- subset(combined_search_differences, metric %in% preference_metrics)
search_differences_plot_preference <- combined_search_differences_preference %>%
      ggplot(aes(x = diff_in_rating, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_search_differences_preference[1,'search'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
search_differences_plot_preference

```

View intervals
```{r}
fit_info_search_differences_preference <- combined_search_differences_preference %>% group_by(search, metric) %>% mean_qi(diff_in_rating, .width = c(.95, .5))
fit_info_search_differences_preference
```

```{r echo=FALSE}
ggsave(file="search_rating_differences_confidence.png", plot=search_differences_plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="search_rating_differences_preference.png", plot=search_differences_plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_search_differences <- rbind(fit_info_search_differences_confidence, fit_info_search_differences_preference)
write.csv(fit_info_search_differences,"../plot_data/comparisons/user_response/search_rating_differences.csv", row.names = FALSE)

```






### Differences in Oracle
```{r}
combined_oracle_differences <- rbind(oracle_differences$confidence_udata, oracle_differences$confidence_ans, oracle_differences$efficiency, oracle_differences$ease_of_use, oracle_differences$utility, oracle_differences$overall)

combined_oracle_differences$metric <- factor(combined_oracle_differences$metric, levels=rev(analyses))

combined_oracle_differences_confidence <- subset(combined_oracle_differences, metric %in% confidence_metrics)
oracle_differences_plot_confidence <- combined_oracle_differences_confidence %>%
      ggplot(aes(x = diff_in_rating, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_oracle_differences_confidence[1,'oracle'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()

oracle_differences_plot_confidence
```

View intervals
```{r}
fit_info_oracle_differences_confidence <- combined_oracle_differences_confidence %>% group_by(oracle, metric) %>% mean_qi(diff_in_rating, .width = c(.95, .5))
fit_info_oracle_differences_confidence
```

```{r}
combined_oracle_differences_preference <- subset(combined_oracle_differences, metric %in% preference_metrics)
oracle_differences_plot_preference <- combined_oracle_differences_preference %>%
      ggplot(aes(x = diff_in_rating, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_oracle_differences_preference[1,'oracle'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
oracle_differences_plot_preference

```

View intervals
```{r}
fit_info_oracle_differences_preference <- combined_oracle_differences_preference %>% group_by(oracle, metric) %>% mean_qi(diff_in_rating, .width = c(.95, .5))
fit_info_oracle_differences_preference
```

```{r echo=FALSE}
ggsave(file="oracle_rating_differences_confidence.png", plot=oracle_differences_plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="oracle_rating_differences_preference.png", plot=oracle_differences_plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_oracle_differences <- rbind(fit_info_oracle_differences_confidence, fit_info_oracle_differences_preference)
write.csv(fit_info_oracle_differences,"../plot_data/comparisons/user_response/oracle_rating_differences.csv", row.names = FALSE)

```

### dfs compassql vs bfs dziban
```{r}
combined_alg_differences <- rbind(alg_differences$confidence_udata, alg_differences$confidence_ans, alg_differences$efficiency, alg_differences$ease_of_use, alg_differences$utility, alg_differences$overall)
combined_alg_differences$metric <- factor(combined_alg_differences$metric, levels=rev(analyses))


# flip order so that we get bfs - dfs
if(combined_alg_differences[1,'alg']=="dfs - bfs"){
  combined_alg_differences$alg = 'bfs - dfs'
  combined_alg_differences$diff_in_rating = -1 * combined_alg_differences$diff_in_rating
}

combined_alg_differences_confidence <- subset(combined_alg_differences, metric %in% confidence_metrics)
alg_differences_plot_confidence <- combined_alg_differences_confidence %>%
      ggplot(aes(x = diff_in_rating, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_alg_differences_confidence[1,'alg'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()

alg_differences_plot_confidence
```

View intervals
```{r}
fit_info_alg_differences_confidence <- combined_alg_differences_confidence %>% group_by(alg, metric) %>% mean_qi(diff_in_rating, .width = c(.95, .5))
fit_info_alg_differences_confidence
```

```{r}
combined_alg_differences_preference <- subset(combined_alg_differences, metric %in% preference_metrics)
alg_differences_plot_preference <- combined_alg_differences_preference %>%
      ggplot(aes(x = diff_in_rating, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_alg_differences_preference[1,'alg'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
alg_differences_plot_preference

```

View intervals
```{r}
fit_info_alg_differences_preference <- combined_alg_differences_preference %>% group_by(alg, metric) %>% mean_qi(diff_in_rating, .width = c(.95, .5))
fit_info_alg_differences_preference
```

```{r echo=FALSE}
ggsave(file="alg_rating_differences_confidence.png", plot=alg_differences_plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="alg_rating_differences_preference.png", plot=alg_differences_plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_alg_differences <- rbind(fit_info_alg_differences_confidence, fit_info_alg_differences_preference)
write.csv(fit_info_alg_differences,"../plot_data/comparisons/user_response/alg_rating_differences.csv", row.names = FALSE)

```
