---
title: "user_response"
output: html_document
---

### Libraries required for this analysis
```{r libraries, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(fig.align="center") 
library(rstanarm)
library(tidyverse)
library(tidybayes)
library(modelr) 
library(ggplot2)
library(magrittr)  
library(emmeans)
library(bayesplot)
library(brms)
library(gganimate)

theme_set(theme_light())


source('helper_functions.R')

```

In our experiment, we used a visualization recommendation algorithm (composed of one search algorithm and one oracle algorithm) to generate visualizations for the user on one of two datasets.  We then asked the user to evaluate the tool on a variety of metrics (confidence in understanding data, confidence in answer, efficiency, ease of use, utility, and overall).

Given a search algorithm (bfs or dfs), an oracle (compassql or dziban), and a dataset (birdstrikes or movies), we would like to predict a user's average score for a given metric.  In addition, we would like to know if the choice of search algorithm and oracle has any meaningful impact on a user's ratong for these metrics.

### Read in and clean data

```{r data_prep}
analyses = c("confidence.udata", "confidence.ans", "efficiency", "ease.of.use", "utility", "overall")
confidence_metrics = c("confidence.udata", "confidence.ans")
preference_metrics = c("efficiency", "ease.of.use", "utility", "overall")

user_response_data <- read.csv('split_by_participant_groups/ptask_responses.csv')
analyses = c("confidence.udata", "confidence.ans", "efficiency", "ease.of.use", "utility", "overall")
user_response_data[,analyses] <- lapply(user_response_data[,analyses],ordered)
user_response_data <- user_response_data %>%
  mutate(
    dataset = as.factor(dataset),
    oracle = as.factor(oracle),
    search = as.factor(search),
    task = as.factor(task)
  )

models <- list()

search_differences <- list()
oracle_differences <- list()
alg_differences <- list()
participant_group_differences <- list()

seed = 12
```

## Analysis for user responses

### Confidence in Understanding Data:  Building a Model

```{r}
models$confidence_udata <- brm(
    formula = bf(confidence.udata ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
    prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/confidence_udata",
    seed = seed
  )

```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$confidence_udata)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$confidence_udata)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$confidence_udata,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$confidence_udata,
  pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for confidence in understanding the data using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
confidence_udata_plot <- user_response_posterior_draws_plot(user_response_data, models$confidence_udata, NULL, "Oracle/Search Combination", "Rating")
confidence_udata_plot$plot
```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
confidence_udata_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "confidence_udata.png", plot = confidence_udata_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(confidence_udata_plot$intervals, "../plot_data/posterior_draws/user_response/confidence_udata.csv", row.names = FALSE)
```

### Confidence in Understanding Data: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
confidence_udata_predictive_data <- user_response_data %>% add_predicted_draws(models$confidence_udata, seed = seed, re_formula = NA) 
confidence_udata_predictive_data$alg <- paste(confidence_udata_predictive_data$search, confidence_udata_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$confidence_udata <- user_response_expected_diff_in_mean_plot(confidence_udata_predictive_data, "search", "confidence.udata", "Difference in Average Confidence in Understanding Data (Rating)", "Task", NULL)
search_differences$confidence_udata$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$confidence_udata <- user_response_expected_diff_in_mean_plot(confidence_udata_predictive_data, "oracle", "confidence.udata", "Difference in Average Confidence in Understanding Data (Rating)", "Task", NULL)
oracle_differences$confidence_udata$plot
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}
confidence_udata_predictive_data_subset <- subset(confidence_udata_predictive_data, alg %in% c("dfs compassql", "bfs dziban"))

alg_differences$confidence_udata <- user_response_expected_diff_in_mean_plot(confidence_udata_predictive_data_subset, "alg", "confidence.udata", "Difference in Average Confidence in Understanding Data (Rating)", "Task", NULL)
alg_differences$confidence_udata$plot
```



Differences in user score by participant group
``` {r}
participant_group_differences$confidence_udata <- user_response_expected_diff_in_mean_plot(confidence_udata_predictive_data, "participant_group", "confidence.udata", "Difference in Average Confidence in Understanding Data (Rating)", "Task", NULL)
participant_group_differences$confidence_udata$plot
```


### Confidence in Answer:  Building a Model

```{r}
models$confidence_ans <- brm(
    formula = bf(confidence.ans ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
    prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/confidence_ans",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$confidence_ans)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$confidence_ans)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$confidence_ans,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$confidence_ans,
  pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for confidence in answer using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
confidence_ans_plot <- user_response_posterior_draws_plot(user_response_data, models$confidence_ans, NULL, "Oracle/Search Combination", "Rating")
confidence_ans_plot$plot
```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
confidence_ans_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "confidence_ans.png", plot = confidence_ans_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(confidence_ans_plot$intervals, "../plot_data/posterior_draws/user_response/confidence_ans.csv", row.names = FALSE)
```

### Confidence in Answer: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
confidence_ans_predictive_data <- user_response_data %>% add_predicted_draws(models$confidence_ans, seed = seed, re_formula = NA) 
confidence_ans_predictive_data$alg <- paste(confidence_ans_predictive_data$search, confidence_ans_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$confidence_ans <- user_response_expected_diff_in_mean_plot(confidence_ans_predictive_data, "search", "confidence.ans", "Difference in Average Confidence in Answer (Rating)", "Task", NULL)
search_differences$confidence_ans$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$confidence_ans <- user_response_expected_diff_in_mean_plot(confidence_ans_predictive_data, "oracle", "confidence.ans", "Difference in Average Confidence in Answer (Rating)", "Task", NULL)
oracle_differences$confidence_ans$plot
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}
confidence_ans_predictive_data_subset <- subset(confidence_ans_predictive_data, alg %in% c("dfs compassql", "bfs dziban"))
alg_differences$confidence_ans <- user_response_expected_diff_in_mean_plot(confidence_ans_predictive_data_subset, "alg", "confidence.ans", "Difference in Average Confidence in Answer (Rating)", "Task", NULL)
alg_differences$confidence_ans$plot
```

Differences in user score by participant group
``` {r}
participant_group_differences$confidence_ans <- user_response_expected_diff_in_mean_plot(confidence_ans_predictive_data, "participant_group", "confidence.ans", "Difference in Average Confidence in Answer (Rating)", "Task", NULL)
participant_group_differences$confidence_ans$plot
```



### Efficiency:  Building a Model

```{r}
filename = "efficiency"
models$efficiency <- brm(
    formula = bf(efficiency ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/efficiency",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$efficiency)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$efficiency)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$efficiency,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$efficiency,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for efficiency using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
efficiency_plot <- user_response_posterior_draws_plot(user_response_data, models$efficiency, NULL, "Oracle/Search Combination", "Rating")
efficiency_plot$plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
efficiency_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "efficiency.png", plot = efficiency_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(efficiency_plot$intervals, "../plot_data/posterior_draws/user_response/efficiency.csv", row.names = FALSE)
```

### Efficiency: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
efficiency_predictive_data <- user_response_data %>% add_predicted_draws(models$efficiency, seed = seed, re_formula = NA) 
efficiency_predictive_data$alg <- paste(efficiency_predictive_data$search, efficiency_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$efficiency <- user_response_expected_diff_in_mean_plot(efficiency_predictive_data, "search", "efficiency", "Difference in Average Efficiency (Rating)", "Task", NULL)
search_differences$efficiency$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$efficiency <- user_response_expected_diff_in_mean_plot(efficiency_predictive_data, "oracle", "efficiency", "Difference in Average Efficiency (Rating)", "Task", NULL)
oracle_differences$efficiency$plot
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}
efficiency_predictive_data_data_subset <- subset(efficiency_predictive_data, alg %in% c("dfs compassql", "bfs dziban"))
alg_differences$efficiency <- user_response_expected_diff_in_mean_plot(efficiency_predictive_data_data_subset, "alg", "efficiency", "Difference in Average Efficiency (Rating)", "Task", NULL)
alg_differences$efficiency$plot
```
Differences in user score by participant group
``` {r}
participant_group_differences$efficiency <- user_response_expected_diff_in_mean_plot(efficiency_predictive_data, "participant_group", "efficiency", "Difference in Average Efficiency (Rating)", "Task", NULL)
participant_group_differences$efficiency$plot
```




### Ease of Use:  Building a Model

```{r}
models$ease_of_use <- brm(
    formula = bf(ease.of.use ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/ease_of_use",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$ease_of_use)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$ease_of_use)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$ease_of_use,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$ease_of_use,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for ease of use using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
ease_of_use_plot <- user_response_posterior_draws_plot(user_response_data, models$ease_of_use, NULL, "Oracle/Search Combination", "Rating")
ease_of_use_plot$plot
```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
ease_of_use_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "ease_of_use.png", plot = ease_of_use_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(ease_of_use_plot$intervals, "../plot_data/posterior_draws/user_response/ease_of_use.csv", row.names = FALSE)
```

### Ease of Use: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
ease_of_use_predictive_data <- user_response_data %>% add_predicted_draws(models$ease_of_use, seed = seed, re_formula = NA) 
ease_of_use_predictive_data$alg <- paste(ease_of_use_predictive_data$search, ease_of_use_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$ease_of_use <- user_response_expected_diff_in_mean_plot(ease_of_use_predictive_data, "search", "ease.of.use", "Difference in Average Ease of Use (Rating)", "Task", NULL)
search_differences$ease_of_use$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$ease_of_use <- user_response_expected_diff_in_mean_plot(ease_of_use_predictive_data, "oracle", "ease.of.use", "Difference in Average Ease of Use (Rating)", "Task", NULL)
oracle_differences$ease_of_use$plot
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}
ease_of_use_predictive_data_subset <- subset(ease_of_use_predictive_data, alg %in% c("dfs compassql", "bfs dziban"))

alg_differences$ease_of_use <- user_response_expected_diff_in_mean_plot(ease_of_use_predictive_data_subset, "alg", "ease.of.use", "Difference in Average Ease of Use (Rating)", "Task", NULL)
alg_differences$ease_of_use$plot
```

Differences in user score by participant group
``` {r}
participant_group_differences$ease_of_use <- user_response_expected_diff_in_mean_plot(ease_of_use_predictive_data, "participant_group", "ease.of.use", "Difference in Average Ease of Use (Rating)", "Task", NULL)
participant_group_differences$ease_of_use$plot
```



### Utility:  Building a Model

```{r}
models$utility <- brm(
    formula = bf(utility ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/utility",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$utility)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$utility)
```

s plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$utility,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$utility,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for Utility using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
utility_plot <- user_response_posterior_draws_plot(user_response_data, models$utility, NULL, "Oracle/Search Combination", "Rating")
utility_plot$plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
utility_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "utility.png", plot = utility_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(utility_plot$intervals, "../plot_data/posterior_draws/user_response/utility.csv", row.names = FALSE)
```

### Utility: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
utility_predictive_data <- user_response_data %>% add_predicted_draws(models$utility, seed = seed, re_formula = NA) 
utility_predictive_data$alg <- paste(utility_predictive_data$search, utility_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$utility <- user_response_expected_diff_in_mean_plot(utility_predictive_data, "search", "utility", "Difference in Average Utility (Rating)", "Task", NULL)
search_differences$utility$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$utility <- user_response_expected_diff_in_mean_plot(utility_predictive_data, "oracle", "utility", "Difference in Average Utility (Rating)", "Task", NULL)
oracle_differences$utility$plot
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}
utility_predictive_data_subset <- subset(utility_predictive_data, alg %in% c("dfs compassql", "bfs dziban"))
alg_differences$utility <- user_response_expected_diff_in_mean_plot(utility_predictive_data_subset, "alg", "utility", "Difference in Average Utility (Rating)", "Task", NULL)
alg_differences$utility$plot

```

Differences in user score by participant group
``` {r}
participant_group_differences$utility <- user_response_expected_diff_in_mean_plot(utility_predictive_data, "participant_group", "utility", "Difference in Average Utility (Rating)", "Task", NULL)
participant_group_differences$utility$plot
```



### Overall:  Building a Model

```{r}
models$overall <- brm(
    formula = bf(overall ~ dataset + oracle * search + task + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/overall",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$overall)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
plot(models$overall)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$overall,
  pars = c("b_Intercept[1]",
           "b_Intercept[2]",
           "b_Intercept[3]",
           "b_Intercept[4]"),
  fixed = TRUE
)
```

```{r}
pairs(
  models$overall,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a average response for Overall using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
overall_plot <- user_response_posterior_draws_plot(user_response_data, models$overall, NULL, "Oracle/Search Combination", "Rating")
overall_plot$plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
overall_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "overall.png", plot = overall_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(overall_plot$intervals, "../plot_data/posterior_draws/user_response/overall.csv", row.names = FALSE)
```

### Overall: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

``` {r}
overall_predictive_data <- user_response_data %>% add_predicted_draws(models$overall, seed = seed, re_formula = NA) 
overall_predictive_data$alg <- paste(overall_predictive_data$search, overall_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$overall <- user_response_expected_diff_in_mean_plot(overall_predictive_data, "search", "overall", "Difference in Average Overall (Rating)", "Task", NULL)
search_differences$overall$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$overall <- overall_predictive_data %>% 
  group_by(oracle, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = oracle) %>%
   rename(diff_in_rating = rating)

oracle_differences$overall$metric = "overall"

oracle_differences$overall %>%
      ggplot(aes(x = diff_in_rating, y = "overall")) +
      xlab(paste0("Expected Difference in Rating (",oracle_differences$overall[1,'oracle'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()

oracle_differences$overall <- user_response_expected_diff_in_mean_plot(overall_predictive_data, "oracle", "overall", "Difference in Average Overall (Rating)", "Task", NULL)
oracle_differences$overall$plot
```


Differences in user score by search and oracle combination (dfs compassql vs bfs dziban only)
``` {r}
overall_predictive_data_subset <- subset(overall_predictive_data, alg %in% c("dfs compassql", "bfs dziban"))
alg_differences$overall <- user_response_expected_diff_in_mean_plot(overall_predictive_data_subset, "alg", "overall", "Difference in Average Overall (Rating)", "Task", NULL)
alg_differences$overall$plot
```


Differences in user score by participant group
``` {r}
participant_group_differences$overall <- user_response_expected_diff_in_mean_plot(overall_predictive_data, "participant_group", "overall", "Difference in Average Overall (Rating)", "Task", NULL)
participant_group_differences$overall$plot
```


## Summary Plots
Putting the all of the plots for search algorithm and oracle differences together, split by whether the rating metric is of type confidence or preference  We'll start with differences in search algorithms.

### Differences in Search Algorithms
```{r}
combined_search_differences <- rbind(
  search_differences$confidence_udata$differences, 
  search_differences$confidence_ans$differences, 
  search_differences$efficiency$differences,
  search_differences$ease_of_use$differences, 
  search_differences$utility$differences, 
  search_differences$overall$differences)

combined_search_differences$metric <- factor(combined_search_differences$metric, levels=rev(analyses))
```

```{r}
combined_search_differences_confidence <- subset(combined_search_differences, metric %in% confidence_metrics)
search_differences_plot_confidence <- combined_search_differences_confidence %>%
      ggplot(aes(x = difference, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_search_differences_confidence[1,'search'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()

search_differences_plot_confidence
```

View intervals
```{r}
fit_info_search_differences_confidence <- combined_search_differences_confidence %>% group_by(search, metric) %>% mean_qi(difference, .width = c(.95, .5))
fit_info_search_differences_confidence
```

```{r}
combined_search_differences_preference <- subset(combined_search_differences, metric %in% preference_metrics)
search_differences_plot_preference <- combined_search_differences_preference %>%
      ggplot(aes(x = difference, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_search_differences_preference[1,'search'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
search_differences_plot_preference

```

View intervals
```{r}
fit_info_search_differences_preference <- combined_search_differences_preference %>% group_by(search, metric) %>% mean_qi(difference, .width = c(.95, .5))
fit_info_search_differences_preference
```

```{r echo=FALSE}
ggsave(file="search_rating_differences_confidence.png", plot=search_differences_plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="search_rating_differences_preference.png", plot=search_differences_plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_search_differences <- rbind(fit_info_search_differences_confidence, fit_info_search_differences_preference)
write.csv(fit_info_search_differences,"../plot_data/comparisons/user_response/search_rating_differences.csv", row.names = FALSE)

```



### Differences in Oracle
```{r}
combined_oracle_differences <- rbind(
  oracle_differences$confidence_udata$differences, 
  oracle_differences$confidence_ans$differences, 
  oracle_differences$efficiency$differences,
  oracle_differences$ease_of_use$differences, 
  oracle_differences$utility$differences, 
  oracle_differences$overall$differences)

combined_oracle_differences$metric <- factor(combined_oracle_differences$metric, levels=rev(analyses))
```

```{r}
combined_oracle_differences_confidence <- subset(combined_oracle_differences, metric %in% confidence_metrics)
oracle_differences_plot_confidence <- combined_oracle_differences_confidence %>%
      ggplot(aes(x = difference, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_oracle_differences_confidence[1,'oracle'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()

oracle_differences_plot_confidence
```

View intervals
```{r}
fit_info_oracle_differences_confidence <- combined_oracle_differences_confidence %>% group_by(oracle, metric) %>% mean_qi(difference, .width = c(.95, .5))
fit_info_oracle_differences_confidence
```

```{r}
combined_oracle_differences_preference <- subset(combined_oracle_differences, metric %in% preference_metrics)
oracle_differences_plot_preference <- combined_oracle_differences_preference %>%
      ggplot(aes(x = difference, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_oracle_differences_preference[1,'oracle'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
oracle_differences_plot_preference

```

View intervals
```{r}
fit_info_oracle_differences_preference <- combined_oracle_differences_preference %>% group_by(oracle, metric) %>% mean_qi(difference, .width = c(.95, .5))
fit_info_oracle_differences_preference
```

```{r echo=FALSE}
ggsave(file="oracle_rating_differences_confidence.png", plot=oracle_differences_plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="oracle_rating_differences_preference.png", plot=oracle_differences_plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_oracle_differences <- rbind(fit_info_oracle_differences_confidence, fit_info_oracle_differences_preference)
write.csv(fit_info_oracle_differences,"../plot_data/comparisons/user_response/oracle_rating_differences.csv", row.names = FALSE)

```

### dfs compassql vs bfs dziban
```{r}
combined_alg_differences <- rbind(
  alg_differences$confidence_udata$differences, 
  alg_differences$confidence_ans$differences, 
  alg_differences$efficiency$differences,
  alg_differences$ease_of_use$differences, 
  alg_differences$utility$differences, 
  alg_differences$overall$differences)

combined_alg_differences$metric <- factor(combined_alg_differences$metric, levels=rev(analyses))
```

```{r}
combined_alg_differences_confidence <- subset(combined_alg_differences, metric %in% confidence_metrics)
alg_differences_plot_confidence <- combined_alg_differences_confidence %>%
      ggplot(aes(x = difference, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_alg_differences_confidence[1,'alg'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()

alg_differences_plot_confidence
```

View intervals
```{r}
fit_info_alg_differences_confidence <- combined_alg_differences_confidence %>% group_by(alg, metric) %>% mean_qi(difference, .width = c(.95, .5))
fit_info_alg_differences_confidence
```

```{r}
combined_alg_differences_preference <- subset(combined_alg_differences, metric %in% preference_metrics)
alg_differences_plot_preference <- combined_alg_differences_preference %>%
      ggplot(aes(x = difference, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_alg_differences_preference[1,'alg'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
alg_differences_plot_preference

```

View intervals
```{r}
fit_info_alg_differences_preference <- combined_alg_differences_preference %>% group_by(alg, metric) %>% mean_qi(difference, .width = c(.95, .5))
fit_info_alg_differences_preference
```

```{r echo=FALSE}
ggsave(file="alg_rating_differences_confidence.png", plot=alg_differences_plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="alg_rating_differences_preference.png", plot=alg_differences_plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_alg_differences <- rbind(fit_info_alg_differences_confidence, fit_info_alg_differences_preference)
write.csv(fit_info_alg_differences,"../plot_data/comparisons/user_response/alg_rating_differences.csv", row.names = FALSE)

```


### Differences in Participant Group
```{r}
combined_participant_group_differences <- rbind(
  participant_group_differences$confidence_udata$differences, 
  participant_group_differences$confidence_ans$differences, 
  participant_group_differences$efficiency$differences,
  participant_group_differences$ease_of_use$differences, 
  participant_group_differences$utility$differences, 
  participant_group_differences$overall$differences)

combined_participant_group_differences$metric <- factor(combined_participant_group_differences$metric, levels=rev(analyses))
```

```{r}
combined_participant_group_differences_confidence <- subset(combined_participant_group_differences, metric %in% confidence_metrics)
participant_group_differences_plot_confidence <- combined_participant_group_differences_confidence %>%
      ggplot(aes(x = difference, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_participant_group_differences_confidence[1,'participant_group'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()

participant_group_differences_plot_confidence
```

View intervals
```{r}
fit_info_participant_group_differences_confidence <- combined_participant_group_differences_confidence %>% group_by(participant_group, metric) %>% mean_qi(difference, .width = c(.95, .5))
fit_info_participant_group_differences_confidence
```

```{r}
combined_participant_group_differences_preference <- subset(combined_participant_group_differences, metric %in% preference_metrics)
participant_group_differences_plot_preference <- combined_participant_group_differences_preference %>%
      ggplot(aes(x = difference, y = metric)) +
      ylab("Confidence") +
      xlab(paste0("Expected Difference in Rating (",combined_participant_group_differences_preference[1,'participant_group'],")")) +
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()
participant_group_differences_plot_preference

```

View intervals
```{r}
fit_info_participant_group_differences_preference <- combined_participant_group_differences_preference %>% group_by(participant_group, metric) %>% mean_qi(difference, .width = c(.95, .5))
fit_info_participant_group_differences_preference
```

```{r echo=FALSE}
ggsave(file="participant_group_rating_differences_confidence.png", plot=participant_group_differences_plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="participant_group_rating_differences_preference.png", plot=participant_group_differences_plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_participant_group_differences <- rbind(fit_info_participant_group_differences_confidence, fit_info_participant_group_differences_preference)
write.csv(fit_info_participant_group_differences,"../plot_data/comparisons/user_response/participant_group_rating_differences.csv", row.names = FALSE)

```
