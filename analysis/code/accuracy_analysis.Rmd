---
title: "accuracy_analysis.Rmd"
output: html_document
---

### Libraries required for this analysis
```{r libraries, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(fig.align="center") 
library(rstanarm)
library(tidyverse)
library(tidybayes)
library(modelr) 
library(ggplot2)
library(magrittr)  
library(emmeans)
library(bayesplot)
library(brms)
library(gganimate)

theme_set(theme_light())
```

In our experiment, we used a visualization recommendation algorithm (composed of one search algorithm and one oracle algorithm) to generate visualizations for the user on one of two datasets.  We then measured the user's accuracy on two tasks: Find Extremum and Retrieve Value.

Given a search algorithm (bfs or dfs), an oracle (compassql or dziban), and a dataset (birdstrikes or movies), we would like to predict a user's chance of answering the Find Extremum task and the Retrieve Value tasks correctly.  In addition, we would like to know if the choice of search algorithm and oracle has any meaningful impact on a user's accuracy for these two tasks.

### Read in and clean data
```{r}
accuracy_data = read.csv('processed_accuracy_split.csv')
accuracy_data$oracle = as.factor(accuracy_data$oracle)
accuracy_data$search = as.factor(accuracy_data$search)
accuracy_data$dataset = as.factor(accuracy_data$dataset)

models <- list()

draw_data <- list()

search_differences <- list()
oracle_differences <- list()

seed = 12
```

## Building a Model for Accuracy Analysis
```{r}
model <- brm(
  bf(
    accuracy ~ 0 + Intercept + oracle * search + dataset + task
    + (1 | participant_id)
  ),
  data = accuracy_data,
  prior = c(prior(normal(0.5, .5), class = "b", coef = "Intercept"),
            prior(normal(0, 2.5), class = "b")),
  family = bernoulli(link = "logit"),
  warmup = 500,
  iter = 3000,
  chains = 2,
  cores = 2, 
  control = list(adapt_delta = 0.9),
  seed = seed,
  file = "model_accuracy"
)
```

### Diagnostics + Model Evaluation

In the summary table, we want to see Rhat values close to 1.0 and Bulk_ESS in the thousands.
``` {r}
summary(model)
```

Trace plots help us check whether there is evidence of non-convergence for model.
```{r}
plot(model)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r fig.height=8, fig.width=8}
pairs(
  model,
  pars = c("b_Intercept",
            "b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue"),
  fixed = TRUE
)
```

```{r}
pp_check(model, type = "dens_overlay", nsamples = 100)
```

A confusion matrix can be used to check our correct classification rate (a useful measure to see how well our model fits our data).
```{r}
pred <- predict(model, type = "response")
pred <- if_else(pred[,1] > 0.5, 1, 0)
confusion_matrix <- table(pred, pull(accuracy_data, accuracy)) 
confusion_matrix
```

Visualization of parameter effects via draws from our model posterior.  The thicker line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
draw_data <- accuracy_data %>%
  add_fitted_draws(model, seed = seed, re_formula = NA, scale = "response") %>%
  group_by(search, oracle, dataset, task, .draw)

draw_data$condition <- paste(draw_data$oracle, draw_data$search, sep="_")

draw_plot <- draw_data %>% ggplot(aes(
    x = .value,
    y = condition,
    fill = dataset,
    alpha = 0.5
  )) + stat_halfeye(.width = c(.95, .5)) +
  facet_grid(. ~ task) +
    labs(x = "Predicted Accuracy (p_correct)", y = "Oracle/Search Combination") 

draw_plot
```
Since the credible intervals on our plot overlap, we can use mean_qi to get the numeric boundaries for the different intervals.
```{r}
fit_info <-  draw_data %>% group_by(search, oracle, dataset, task) %>% mean_qi(.value, .width = c(.95, .5))
fit_info
```

``` {r echo = FALSE}
# save the outputted plots and files
 ggsave(
    file = "plot.png",
    plot = draw_plot,
    path = "../plots/posterior_draws/accuracy"
  )
  write.csv(fit_info,
            paste("../plot_data/posterior_draws/accuracy/fit.csv", sep = ""),
            row.names = FALSE)
```

### Differences Between Conditions
Next, we want to see if there is any significant difference in accuracy between the two search algorithms (bfs and dfs) and the two oracles (dzbian and compassql).

Differences in search algorithms:
``` {r}
predictive_data  <- accuracy_data %>%
    add_fitted_draws(model, seed = seed, re_formula = NA, scale = "response")

search_differences <- predictive_data  %>%
    group_by(search, dataset, task, .draw) %>%
    summarize(accuracy = weighted.mean(.value)) %>%
    compare_levels(accuracy, by = search) %>%
    rename(difference_in_accuracy = accuracy)

search_differences_plot <- search_differences %>%
      ggplot(aes(x = difference_in_accuracy, y = task, fill = dataset, alpha = 0.5)) +
      xlab(paste0("Expected Difference in Accuracy (",search_differences[1,'search'],")")) + 
      ylab("Task")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      facet_grid(. ~ dataset) + 
      theme_minimal()

search_differences_plot
```
We can double-check the boundaries of the credible intervals to be sure whether or not the interval contains zero.
```{r}
search_intervals <- search_differences %>% mean_qi(difference_in_accuracy, .width = c(.95, .5))
search_intervals
```


Differences in oracle:
``` {r}
oracle_differences <- predictive_data  %>%
    group_by(oracle, dataset, task, .draw) %>%
    summarize(accuracy = weighted.mean(.value)) %>%
    compare_levels(accuracy, by = oracle) %>%
    rename(difference_in_accuracy = accuracy)

oracle_differences_plot <- oracle_differences %>%
      ggplot(aes(x = difference_in_accuracy, y = task, fill = dataset, alpha = 0.5)) +
      xlab(paste0("Expected Difference in Accuracy (",oracle_differences[1,'oracle'],")")) + 
      ylab("Task")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
    facet_grid(. ~ dataset) +
      theme_minimal()

oracle_differences_plot
  
```
We can double-check the boundaries of the credible intervals to be sure whether or not the interval contains zero.
```{r}
oracle_intervals <- oracle_differences %>% mean_qi(difference_in_accuracy, .width = c(.95, .5))
oracle_intervals
```

```{r echo = FALSE}
ggsave(file="search_accuracy_differences.png", plot=search_differences_plot, path = "../plots/comparisons/accuracy", width = 7, height = 7)
write.csv(search_intervals, "../plot_data/comparisons/accuracy/search_accuracy_differences.csv", row.names = FALSE)

ggsave(file="oracle_accuracy_differences.png", plot=oracle_differences_plot, path = "../plots/comparisons/accuracy", width = 7, height = 7)
write.csv(oracle_intervals, "../plot_data/comparisons/accuracy/oracle_accuracy_differences.csv", row.names = FALSE)
```
