---
title: "user_response"
output: html_document
---

### Libraries required for this analysis
```{r libraries, message=FALSE, warning=FALSE}

knitr::opts_chunk$set(fig.align="center") 
library(rstanarm)
library(tidyverse)
library(tidybayes)
library(modelr) 
library(ggplot2)
library(magrittr)  
library(emmeans)
library(bayesplot)
library(brms)
library(gganimate)

theme_set(theme_light())


source('helper_functions.R')

```

In our experiment, we used a visualization recommendation algorithm (composed of one search algorithm and one oracle algorithm) to generate visualizations for the user on one of two datasets.  We then asked the user to evaluate the tool on a variety of metrics (confidence in understanding data, confidence in answer, efficiency, ease of use, utility, and overall).

Given a search algorithm (bfs or dfs), an oracle (CompassQL or dziban), and a dataset (birdstrikes or movies), we would like to predict a user's score for a given metric.  In addition, we would like to know if the choice of search algorithm and oracle, as well as participant group (student or professional) has any meaningful impact on a user's rating for these metrics.

Our weakly-informative prior (normal(0.26, 1.26)) was derived from pilot studies, and it summarizes the user rating for each metric.  Because our pilot study was small, we chose to aggregate our data (rather than deriving separate priors for each metric) to minimize the effect of biases.

Since ratings can have values between -2 and 2 inclusive, we perform ordinal regression.

### Read in and clean data

```{r data_prep}
analyses = c("confidence.udata", "confidence.ans", "efficiency", "ease.of.use", "utility", "overall")
confidence_metrics = c("confidence.udata", "confidence.ans")
preference_metrics = c("efficiency", "ease.of.use", "utility", "overall")

user_response_data <- read.csv('data/ptask_responses.csv')
analyses = c("confidence.udata", "confidence.ans", "efficiency", "ease.of.use", "utility", "overall")

user_response_data$oracle<- gsub('compassql', 'CompassQL', user_response_data$oracle)
user_response_data$oracle<- gsub('dziban', 'Dziban', user_response_data$oracle)

user_response_data$search<- gsub('bfs', 'BFS', user_response_data$search)
user_response_data$search<- gsub('dfs', 'DFS', user_response_data$search)

user_response_data[,analyses] <- lapply(user_response_data[,analyses],ordered)
user_response_data <- user_response_data %>%
  mutate(
    dataset = as.factor(dataset),
    oracle = as.factor(oracle),
    search = as.factor(search),
    task = as.factor(task)
  )

models <- list()

search_differences <- list()
oracle_differences <- list()
alg_differences <- list()
participant_group_differences <- list()

seed = 12
```

## Analysis for user responses

### Confidence in Understanding Data:  Building a Model

```{r}
models$confidence_udata <- brm(
    formula = bf(confidence.udata ~ oracle * search + dataset + task + participant_group + (1 | participant_id)),
    family = cumulative("probit"),
    prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/confidence_udata",
    seed = seed
  )

```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$confidence_udata)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
# plot(models$confidence_udata)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).

```{r}
pairs(
  models$confidence_udata,
  pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a response for confidence in understanding the data using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
confidence_udata_plot <- user_response_posterior_draws_plot(user_response_data, models$confidence_udata, NULL, "Oracle/Search Combination", "Rating")
confidence_udata_plot$plot
```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
confidence_udata_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "confidence_udata.png", plot = confidence_udata_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(confidence_udata_plot$intervals, "../plot_data/posterior_draws/user_response/confidence_udata.csv", row.names = FALSE)
```

### Confidence in Understanding Data: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and CompassQL).

``` {r}
confidence_udata_predictive_data <- user_response_data %>% add_predicted_draws(models$confidence_udata, seed = seed, re_formula = NA)
confidence_udata_predictive_data$alg <- paste(confidence_udata_predictive_data$search, confidence_udata_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$confidence_udata <- user_response_diff_plot(confidence_udata_predictive_data, "search", "confidence.udata", "Difference in Confidence in Understanding Data Rating", "Task", NULL)
search_differences$confidence_udata$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$confidence_udata <- user_response_diff_plot(confidence_udata_predictive_data, "oracle", "confidence.udata", "Difference in Confidence in Understanding Data Rating", "Task", NULL)
oracle_differences$confidence_udata$plot
```


Differences in user score by search and oracle combination (DFS CompassQL vs BFS Dziban only)
``` {r}
confidence_udata_predictive_data_subset <- subset(confidence_udata_predictive_data, alg %in% c("DFS CompassQL", "BFS Dziban"))

alg_differences$confidence_udata <- user_response_diff_plot(confidence_udata_predictive_data_subset, "alg", "confidence.udata", "Difference in Confidence in Understanding Data Rating", "Task", NULL)
alg_differences$confidence_udata$plot
```


Differences in user score by participant group
``` {r}
participant_group_differences$confidence_udata <- user_response_diff_plot(confidence_udata_predictive_data, "participant_group", "confidence.udata", "Difference in Confidence in Understanding Data Rating", "Task", NULL)
participant_group_differences$confidence_udata$plot
```


### Confidence in Answer:  Building a Model

```{r}
models$confidence_ans <- brm(
    formula = bf(confidence.ans ~ oracle * search + dataset + task + participant_group + (1 | participant_id)),
    family = cumulative("probit"),
    prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/confidence_ans",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$confidence_ans)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
# plot(models$confidence_ans)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).

```{r}
pairs(
  models$confidence_ans,
  pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a response for confidence in answer using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
confidence_ans_plot <- user_response_posterior_draws_plot(user_response_data, models$confidence_ans, NULL, "Oracle/Search Combination", "Rating")
confidence_ans_plot$plot
```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
confidence_ans_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "confidence_ans.png", plot = confidence_ans_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(confidence_ans_plot$intervals, "../plot_data/posterior_draws/user_response/confidence_ans.csv", row.names = FALSE)
```

### Confidence in Answer: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and CompassQL).

``` {r}
confidence_ans_predictive_data <- user_response_data %>% add_predicted_draws(models$confidence_ans, seed = seed, re_formula = NA) 
confidence_ans_predictive_data$alg <- paste(confidence_ans_predictive_data$search, confidence_ans_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$confidence_ans <- user_response_diff_plot(confidence_ans_predictive_data, "search", "confidence.ans", "Difference in Confidence in Answer Rating", "Task", NULL)
search_differences$confidence_ans$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$confidence_ans <- user_response_diff_plot(confidence_ans_predictive_data, "oracle", "confidence.ans", "Difference in Confidence in Answer Rating", "Task", NULL)
oracle_differences$confidence_ans$plot
```


Differences in user score by search and oracle combination (DFS CompassQL vs BFS Dziban only)
``` {r}
confidence_ans_predictive_data_subset <- subset(confidence_ans_predictive_data, alg %in% c("DFS CompassQL", "BFS Dziban"))
alg_differences$confidence_ans <- user_response_diff_plot(confidence_ans_predictive_data_subset, "alg", "confidence.ans", "Difference in Confidence in Answer Rating", "Task", NULL)
alg_differences$confidence_ans$plot
```

Differences in user score by participant group
``` {r}
participant_group_differences$confidence_ans <- user_response_diff_plot(confidence_ans_predictive_data, "participant_group", "confidence.ans", "Difference in Confidence in Answer Rating", "Task", NULL)
participant_group_differences$confidence_ans$plot
```


### Efficiency:  Building a Model

```{r}
filename = "efficiency"
models$efficiency <- brm(
    formula = bf(efficiency ~ oracle * search + dataset + task + participant_group + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/efficiency",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$efficiency)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
# plot(models$efficiency)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$efficiency,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a response for efficiency using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
efficiency_plot <- user_response_posterior_draws_plot(user_response_data, models$efficiency, NULL, "Oracle/Search Combination", "Rating")
efficiency_plot$plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
efficiency_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "efficiency.png", plot = efficiency_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(efficiency_plot$intervals, "../plot_data/posterior_draws/user_response/efficiency.csv", row.names = FALSE)
```

### Efficiency: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and CompassQL).

``` {r}
efficiency_predictive_data <- user_response_data %>% add_predicted_draws(models$efficiency, seed = seed, re_formula = NA) 
efficiency_predictive_data$alg <- paste(efficiency_predictive_data$search, efficiency_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$efficiency <- user_response_diff_plot(efficiency_predictive_data, "search", "efficiency", "Difference in Efficiency Rating", "Task", NULL)
search_differences$efficiency$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$efficiency <- user_response_diff_plot(efficiency_predictive_data, "oracle", "efficiency", "Difference in Efficiency Rating", "Task", NULL)
oracle_differences$efficiency$plot
```


Differences in user score by search and oracle combination (DFS CompassQL vs BFS Dziban only)
``` {r}
efficiency_predictive_data_data_subset <- subset(efficiency_predictive_data, alg %in% c("DFS CompassQL", "BFS Dziban"))
alg_differences$efficiency <- user_response_diff_plot(efficiency_predictive_data_data_subset, "alg", "efficiency", "Difference in Efficiency Rating", "Task", NULL)
alg_differences$efficiency$plot
```
Differences in user score by participant group
``` {r}
participant_group_differences$efficiency <- user_response_diff_plot(efficiency_predictive_data, "participant_group", "efficiency", "Difference in Efficiency Rating", "Task", NULL)
participant_group_differences$efficiency$plot
```


### Ease of Use:  Building a Model

```{r}
models$ease_of_use <- brm(
    formula = bf(ease.of.use ~ oracle * search + dataset + task + participant_group + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/ease_of_use",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$ease_of_use)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
# plot(models$ease_of_use)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).

```{r}
pairs(
  models$ease_of_use,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a response for ease of use using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
ease_of_use_plot <- user_response_posterior_draws_plot(user_response_data, models$ease_of_use, NULL, "Oracle/Search Combination", "Rating")
ease_of_use_plot$plot
```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
ease_of_use_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "ease_of_use.png", plot = ease_of_use_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(ease_of_use_plot$intervals, "../plot_data/posterior_draws/user_response/ease_of_use.csv", row.names = FALSE)
```

### Ease of Use: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and CompassQL).

``` {r}
ease_of_use_predictive_data <- user_response_data %>% add_predicted_draws(models$ease_of_use, seed = seed, re_formula = NA) 
ease_of_use_predictive_data$alg <- paste(ease_of_use_predictive_data$search, ease_of_use_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$ease_of_use <- user_response_diff_plot(ease_of_use_predictive_data, "search", "ease.of.use", "Difference in Ease of Use Rating", "Task", NULL)
search_differences$ease_of_use$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$ease_of_use <- user_response_diff_plot(ease_of_use_predictive_data, "oracle", "ease.of.use", "Difference in Ease of Use Rating", "Task", NULL)
oracle_differences$ease_of_use$plot
```


Differences in user score by search and oracle combination (DFS CompassQL vs BFS Dziban only)
``` {r}
ease_of_use_predictive_data_subset <- subset(ease_of_use_predictive_data, alg %in% c("DFS CompassQL", "BFS Dziban"))

alg_differences$ease_of_use <- user_response_diff_plot(ease_of_use_predictive_data_subset, "alg", "ease.of.use", "Difference in Ease of Use Rating", "Task", NULL)
alg_differences$ease_of_use$plot
```

Differences in user score by participant group
``` {r}
participant_group_differences$ease_of_use <- user_response_diff_plot(ease_of_use_predictive_data, "participant_group", "ease.of.use", "Difference in Ease of Use Rating", "Task", NULL)
participant_group_differences$ease_of_use$plot
```



### Utility:  Building a Model

```{r}
models$utility <- brm(
    formula = bf(utility ~ oracle * search + dataset + task + participant_group + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/utility",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$utility)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
# plot(models$utility)
```

s plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).
```{r}
pairs(
  models$utility,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a response for Utility using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
utility_plot <- user_response_posterior_draws_plot(user_response_data, models$utility, NULL, "Oracle/Search Combination", "Rating")
utility_plot$plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
utility_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "utility.png", plot = utility_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(utility_plot$intervals, "../plot_data/posterior_draws/user_response/utility.csv", row.names = FALSE)
```

### Utility: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and CompassQL).

``` {r}
utility_predictive_data <- user_response_data %>% add_predicted_draws(models$utility, seed = seed, re_formula = NA) 
utility_predictive_data$alg <- paste(utility_predictive_data$search, utility_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$utility <- user_response_diff_plot(utility_predictive_data, "search", "utility", "Difference in Utility Rating", "Task", NULL)
search_differences$utility$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$utility <- user_response_diff_plot(utility_predictive_data, "oracle", "utility", "Difference in Utility Rating", "Task", NULL)
oracle_differences$utility$plot
```


Differences in user score by search and oracle combination (DFS CompassQL vs BFS Dziban only)
``` {r}
utility_predictive_data_subset <- subset(utility_predictive_data, alg %in% c("DFS CompassQL", "BFS Dziban"))
alg_differences$utility <- user_response_diff_plot(utility_predictive_data_subset, "alg", "utility", "Difference in Utility Rating", "Task", NULL)
alg_differences$utility$plot

```

Differences in user score by participant group
``` {r}
participant_group_differences$utility <- user_response_diff_plot(utility_predictive_data, "participant_group", "utility", "Difference in Utility Rating", "Task", NULL)
participant_group_differences$utility$plot
```



### Overall:  Building a Model

```{r}
models$overall <- brm(
    formula = bf(overall ~ oracle * search + dataset + task + participant_group + (1 | participant_id)),
    family = cumulative("probit"),
   prior = prior(normal(0.26, 1.26), class = Intercept),
    chains = 2,
    cores = 2,
    iter = 2500,
    warmup = 1000,
    data = user_response_data,
    control = list(adapt_delta = 0.99),
    file = "models/overall",
    seed = seed
  )
```

Check some diagnostics regarding our model.  Rhat should be close to 1 and Bulk_ESS should be in the thousands.
```{r}
summary(models$overall)
```

Trace plots help us check whether there is evidence of non-convergence for our model.
```{r}
# plot(models$overall)
```

In our pairs plots, we want to make sure we don't have highly correlated parameters (highly correlated parameters means that our model has difficulty differentiating the effect of such parameters).

```{r}
pairs(
  models$overall,
   pars = c("b_datasetmovies",
           "b_oracledziban",
           "b_searchdfs",
           "b_task2.RetrieveValue",
           "b_task3.Prediction",
           "b_task4.Exploration"),
  fixed = TRUE
)

```

We now look at a response for Overall using different combinations of search and oracle via draws from the model posterior.  The thicker, shorter line represents the 95% credible interval, while the thinner, longer line represents the 50% credible interval.
```{r}
overall_plot <- user_response_posterior_draws_plot(user_response_data, models$overall, NULL, "Oracle/Search Combination", "Rating")
overall_plot$plot

```
We can get the numeric values of the interval boundaries shown above with mean_qi
```{r}
overall_plot$intervals
```

```{r echo=FALSE}
# save the plot and intervals
 ggsave(file = "overall.png", plot = overall_plot$plot, path = "../plots/posterior_draws/user_response")
  write.csv(overall_plot$intervals, "../plot_data/posterior_draws/user_response/overall.csv", row.names = FALSE)
```

### Overall: Differences Between Conditions
Next, we want to see if there is any significant difference in completion time between the two search algorithms (bfs and dfs) and the two oracles (dzbian and CompassQL).

``` {r}
overall_predictive_data <- user_response_data %>% add_predicted_draws(models$overall, seed = seed, re_formula = NA) 
overall_predictive_data$alg <- paste(overall_predictive_data$search, overall_predictive_data$oracle)
```

Differences in user score by search algorithm.
``` {r}
search_differences$overall <- user_response_diff_plot(overall_predictive_data, "search", "overall", "Difference in Overall Rating", "Task", NULL)
search_differences$overall$plot
```

Differences in user score by oracle.
``` {r}
oracle_differences$overall <- overall_predictive_data %>% 
  group_by(oracle, .draw) %>%
   summarize(rating = weighted.mean(as.numeric(.prediction))) %>%
   compare_levels(rating, by = oracle) %>%
   rename(diff_in_rating = rating)

oracle_differences$overall$metric = "overall"

oracle_differences$overall %>%
      ggplot(aes(x = diff_in_rating, y = "overall")) +
      xlab(paste0("Expected Difference in Rating (",oracle_differences$overall[1,'oracle'],")")) + 
      ylab("Condition")+
      stat_halfeye(.width = c(.95, .5)) +
      geom_vline(xintercept = 0, linetype = "longdash") +
      theme_minimal()

oracle_differences$overall <- user_response_diff_plot(overall_predictive_data, "oracle", "overall", "Difference in Overall Rating", "Task", NULL)
oracle_differences$overall$plot
```


Differences in user score by search and oracle combination (DFS CompassQL vs BFS Dziban only)
``` {r}
overall_predictive_data_subset <- subset(overall_predictive_data, alg %in% c("DFS CompassQL", "BFS Dziban"))
alg_differences$overall <- user_response_diff_plot(overall_predictive_data_subset, "alg", "overall", "Difference in Overall Rating", "Task", NULL)
alg_differences$overall$plot
```


Differences in user score by participant group
``` {r}
participant_group_differences$overall <- user_response_diff_plot(overall_predictive_data, "participant_group", "overall", "Difference in Overall Rating", "Task", NULL)
participant_group_differences$overall$plot
```


## Summary Plots
Putting the all of the plots for search algorithm and oracle differences together, split by whether the rating metric is of type confidence or preference  We'll start with differences in search algorithms.

### Differences in Search Algorithms
```{r}
combined_search_differences <- rbind(
  search_differences$confidence_udata$differences, 
  search_differences$confidence_ans$differences, 
  search_differences$efficiency$differences,
  search_differences$ease_of_use$differences, 
  search_differences$utility$differences, 
  search_differences$overall$differences)
```


```{r}
search_difference_plots_intervals <- user_response_diff_summary(combined_search_differences, 'search')
search_difference_plots_intervals$plot_confidence
```

View intervals
```{r}
search_difference_plots_intervals$intervals_confidence
```

```{r}
search_difference_plots_intervals$plot_preference
```

View intervals
```{r}
search_difference_plots_intervals$intervals_preference
```

```{r echo=FALSE}
ggsave(file="search_rating_differences_confidence.png", plot=search_difference_plots_intervals$plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="search_rating_differences_preference.png", plot=search_difference_plots_intervals$plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_search_differences <- rbind(search_difference_plots_intervals$intervals_confidence, search_difference_plots_intervals$intervals_preference)
write.csv(fit_info_search_differences,"../plot_data/comparisons/user_response/search_rating_differences.csv", row.names = FALSE)

```



### Differences in Oracle
```{r}
combined_oracle_differences <- rbind(
  oracle_differences$confidence_udata$differences, 
  oracle_differences$confidence_ans$differences, 
  oracle_differences$efficiency$differences,
  oracle_differences$ease_of_use$differences, 
  oracle_differences$utility$differences, 
  oracle_differences$overall$differences)
```

```{r}
oracle_difference_plots_intervals <- user_response_diff_summary(combined_oracle_differences, 'oracle')
oracle_difference_plots_intervals$plot_confidence
```

View intervals
```{r}
oracle_difference_plots_intervals$intervals_confidence
```

```{r}
oracle_difference_plots_intervals$plot_preference
```

View intervals
```{r}
oracle_difference_plots_intervals$intervals_preference
```

```{r echo=FALSE}
ggsave(file="oracle_rating_differences_confidence.png", plot=oracle_difference_plots_intervals$plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="oracle_rating_differences_preference.png", plot=oracle_difference_plots_intervals$plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_oracle_differences <- rbind(oracle_difference_plots_intervals$intervals_confidence, oracle_difference_plots_intervals$intervals_preference)
write.csv(fit_info_oracle_differences,"../plot_data/comparisons/user_response/oracle_rating_differences.csv", row.names = FALSE)

```

### DFS CompassQL vs BFS Dziban
```{r}
combined_alg_differences <- rbind(
  alg_differences$confidence_udata$differences, 
  alg_differences$confidence_ans$differences, 
  alg_differences$efficiency$differences,
  alg_differences$ease_of_use$differences, 
  alg_differences$utility$differences, 
  alg_differences$overall$differences)
```

```{r}
alg_difference_plots_intervals <- user_response_diff_summary(combined_alg_differences, 'alg')
alg_difference_plots_intervals$plot_confidence
```

View intervals
```{r}
alg_difference_plots_intervals$intervals_confidence
```

```{r}
alg_difference_plots_intervals$plot_preference
```

View intervals
```{r}
alg_difference_plots_intervals$intervals_preference
```

```{r echo=FALSE}
ggsave(file="alg_rating_differences_confidence.png", plot=alg_difference_plots_intervals$plot_confidence, path = "../plots/comparisons/user_response", width = 7, height = 7)
ggsave(file="alg_rating_differences_preference.png", plot=alg_difference_plots_intervals$plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_alg_differences <- rbind(alg_difference_plots_intervals$intervals_confidence, alg_difference_plots_intervals$intervals_preference)
write.csv(fit_info_alg_differences,"../plot_data/comparisons/user_response/alg_rating_differences.csv", row.names = FALSE)

```


### Differences in Participant Group
```{r}
combined_participant_group_differences <- rbind(
  participant_group_differences$confidence_udata$differences, 
  participant_group_differences$confidence_ans$differences, 
  participant_group_differences$efficiency$differences,
  participant_group_differences$ease_of_use$differences, 
  participant_group_differences$utility$differences, 
  participant_group_differences$overall$differences)
```

```{r}
participant_group_difference_plots_intervals <- user_response_diff_summary(combined_participant_group_differences, 'participant_group')
participant_group_difference_plots_intervals$plot_confidence
```

View intervals
```{r}
participant_group_difference_plots_intervals$intervals_confidence
```

```{r}
participant_group_difference_plots_intervals$plot_preference
```

View intervals
```{r}
participant_group_difference_plots_intervals$intervals_preference
```

```{r echo=FALSE}
ggsave(file="participant_group_rating_differences_preference.png", plot=participant_group_difference_plots_intervals$plot_preference, path = "../plots/comparisons/user_response", width = 7, height = 7)

fit_info_participant_group_differences <- rbind(participant_group_difference_plots_intervals$intervals_confidence, participant_group_difference_plots_intervals$intervals_preference)
write.csv(fit_info_participant_group_differences,"../plot_data/comparisons/user_response/participant_group_rating_differences.csv", row.names = FALSE)

```


##Histograms for Response Distributions
Here we plot out the number of responses for each rating (-2 to 2 inclusive) across all of our user metrics (Confidence in Understanding Data, Confidence in Answer, Efficiency, Ease of Use, Utility, and Overall).  Because each user completed 4 tasks, the total number of responses in these graphs is four times the total number of users in our study.
``` {r}
user_response_data$dataset<- gsub('birdstrikes', 'Birdstrikes', user_response_data$dataset)
user_response_data$dataset<- gsub('movies', 'Movies', user_response_data$dataset)

user_response_data %>%
  ggplot(aes(x=confidence.udata)) +
    geom_bar() +
    xlab("Confidence in Understanding Data Rating") +
    ylab("Number of Responses") +
    facet_grid(dataset ~ search+oracle)

user_response_data %>%
  ggplot(aes(x=confidence.ans)) +
    geom_bar() +
    xlab("Confidence in Answer Rating") +
    ylab("Number of Responses") +
    facet_grid(dataset ~ search+oracle)

user_response_data %>%
  ggplot(aes(x=efficiency)) +
    geom_bar() +
    xlab("Efficiency Rating") +
    ylab("Number of Responses") +
    facet_grid(dataset ~ search+oracle)

user_response_data %>%
  ggplot(aes(x=ease.of.use)) +
    geom_bar() +
    xlab("Ease of Use Rating") +
    ylab("Number of Responses") +
    facet_grid(dataset ~ search+oracle)

user_response_data %>%
  ggplot(aes(x=utility)) +
    geom_bar() +
    xlab("Utility Rating") +
    ylab("Number of Responses") +
    facet_grid(dataset ~ search+oracle)

user_response_data %>%
  ggplot(aes(x=overall)) +
    geom_bar() +
    xlab("Overall Rating") +
    ylab("Number of Responses") +
    facet_grid(dataset ~ search+oracle)
```
