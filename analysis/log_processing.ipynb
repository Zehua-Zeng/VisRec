{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "dataset_dict = {\n",
    "    'a': 'movies',\n",
    "    'b': 'birdstrikes'\n",
    "}\n",
    "\n",
    "oracle_dict = {\n",
    "    'c': 'compassql',\n",
    "    'd': 'dziban'\n",
    "}\n",
    "\n",
    "search_algorithm_dict = {\n",
    "    'e': 'bfs',\n",
    "    'f': 'dfs'\n",
    "}\n",
    "\n",
    "task_dict = {\n",
    "    'p1': '1. Find Extremum',\n",
    "    'p2': '2. Retrieve Value',\n",
    "    'p3': '3. Prediction',\n",
    "    'p4': '4. Exploration'\n",
    "}\n",
    "\n",
    "response_to_score_dict = {\n",
    "    'sdisagree': -2,\n",
    "    'disagree': -1,\n",
    "    'neutral': 0,\n",
    "    'agree': 1,\n",
    "    'sagree': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    participant_id      dataset     oracle search      condition  \\\n",
      "0               29       movies  compassql    bfs  compassql_bfs   \n",
      "1               55       movies  compassql    dfs  compassql_dfs   \n",
      "2               42  birdstrikes     dziban    bfs     dziban_bfs   \n",
      "3               58  birdstrikes     dziban    bfs     dziban_bfs   \n",
      "4               22       movies     dziban    bfs     dziban_bfs   \n",
      "..             ...          ...        ...    ...            ...   \n",
      "231             31  birdstrikes     dziban    dfs     dziban_dfs   \n",
      "232             32       movies  compassql    bfs  compassql_bfs   \n",
      "233             41       movies     dziban    bfs     dziban_bfs   \n",
      "234             15       movies     dziban    dfs     dziban_dfs   \n",
      "235             57  birdstrikes     dziban    dfs     dziban_dfs   \n",
      "\n",
      "                  task confidence-udata confidence-ans efficiency ease-of-use  \\\n",
      "0        3. Prediction               -1              0         -1           1   \n",
      "1     1. Find Extremum                1              2          0          -1   \n",
      "2       4. Exploration                1              2          1           1   \n",
      "3        3. Prediction                2              2          2           1   \n",
      "4    2. Retrieve Value                1              2         -1           2   \n",
      "..                 ...              ...            ...        ...         ...   \n",
      "231   1. Find Extremum                0              0          0           0   \n",
      "232  2. Retrieve Value                0              0          1           1   \n",
      "233   1. Find Extremum                1              2         -1           1   \n",
      "234      3. Prediction                2              2          1           1   \n",
      "235   1. Find Extremum                2              2         -2          -2   \n",
      "\n",
      "    utility overall  \n",
      "0        -1      -2  \n",
      "1        -1       0  \n",
      "2         2       1  \n",
      "3         2       2  \n",
      "4        -2       0  \n",
      "..      ...     ...  \n",
      "231       0       0  \n",
      "232       1       1  \n",
      "233      -1       0  \n",
      "234       1       1  \n",
      "235      -2      -2  \n",
      "\n",
      "[236 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "path_to_json = './logs/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('_ptask.json')]\n",
    "# print(json_files) \n",
    "jsons_data = pd.DataFrame(columns=['participant_id',\n",
    "                                   'dataset',\n",
    "                                   'oracle',\n",
    "                                   'search',\n",
    "                                   'condition',\n",
    "                                   'task',\n",
    "                                   'confidence-udata', \n",
    "                                   'confidence-ans', \n",
    "                                   'efficiency',\n",
    "                                   'ease-of-use', \n",
    "                                   'utility', \n",
    "                                   'overall'])\n",
    "\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        \n",
    "        split_filename = js.split('_')\n",
    "        \n",
    "        participant_id = split_filename[0].replace('partcipant','')\n",
    "        experimental_setup = split_filename[1]\n",
    "        dataset = dataset_dict[experimental_setup[0]]\n",
    "        \n",
    "        oracle = oracle_dict[experimental_setup[1]]\n",
    "        search = search_algorithm_dict[experimental_setup[2]]\n",
    "        \n",
    "        condition = oracle+\"_\"+search\n",
    "        \n",
    "        task = task_dict[split_filename[2]]\n",
    "        \n",
    "        confidence_udata = response_to_score_dict[json_text['confidence-udata']]\n",
    "        confidence_ans = response_to_score_dict[json_text['confidence-ans']]\n",
    "        efficiency = response_to_score_dict[json_text['efficiency']]\n",
    "        ease_of_use = response_to_score_dict[json_text['ease-of-use']]\n",
    "        utility = response_to_score_dict[json_text['utility']]\n",
    "        overall = response_to_score_dict[json_text['overall']]\n",
    "        \n",
    "        row = [participant_id, dataset, oracle, search, condition, task, confidence_udata, confidence_ans, efficiency, ease_of_use, utility, overall]\n",
    "        jsons_data.loc[index] = row\n",
    "# now that we have the pertinent json data in our DataFrame let's look at it\n",
    "print(jsons_data)\n",
    "jsons_data.to_csv('processed_ptask_responses.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   participant_id      dataset     oracle search      condition  \\\n",
      "0          pilot3       movies  compassql    dfs  compassql_dfs   \n",
      "1          pilot4       movies     dziban    dfs     dziban_dfs   \n",
      "2          pilot5  birdstrikes     dziban    bfs     dziban_bfs   \n",
      "3          pilot3       movies  compassql    dfs  compassql_dfs   \n",
      "4          pilot4       movies     dziban    dfs     dziban_dfs   \n",
      "5          pilot3       movies  compassql    dfs  compassql_dfs   \n",
      "6          pilot4       movies     dziban    dfs     dziban_dfs   \n",
      "7          pilot4       movies     dziban    dfs     dziban_dfs   \n",
      "8          pilot5  birdstrikes     dziban    bfs     dziban_bfs   \n",
      "9          pilot5  birdstrikes     dziban    bfs     dziban_bfs   \n",
      "10         pilot3       movies  compassql    dfs  compassql_dfs   \n",
      "11         pilot5  birdstrikes     dziban    bfs     dziban_bfs   \n",
      "\n",
      "                 task confidence-udata confidence-ans efficiency ease-of-use  \\\n",
      "0    1. Find Extremum                2              2          2           2   \n",
      "1       3. Prediction               -1             -1         -2           1   \n",
      "2   2. Retrieve Value                2              2          1           1   \n",
      "3      4. Exploration                0              1          1           1   \n",
      "4    1. Find Extremum                1              1         -2           0   \n",
      "5       3. Prediction               -1             -1         -1          -2   \n",
      "6      4. Exploration               -1              0         -1           1   \n",
      "7   2. Retrieve Value               -1              0         -2           1   \n",
      "8       3. Prediction                1              1          0           0   \n",
      "9      4. Exploration                1              0          1           0   \n",
      "10  2. Retrieve Value                1              2         -2          -1   \n",
      "11   1. Find Extremum                1              2          0           1   \n",
      "\n",
      "   utility overall  \n",
      "0        2       2  \n",
      "1       -2      -1  \n",
      "2        0       1  \n",
      "3        1       2  \n",
      "4       -1       1  \n",
      "5       -2       1  \n",
      "6        0       0  \n",
      "7       -2      -1  \n",
      "8        0       1  \n",
      "9        1       1  \n",
      "10      -1       2  \n",
      "11       0       0  \n",
      "confidence-udata\n",
      "0.4166666666666667\n",
      "1.1645001528813153\n",
      "---\n",
      "confidence-ans\n",
      "0.75\n",
      "1.1381803659589922\n",
      "---\n",
      "efficiency\n",
      "-0.4166666666666667\n",
      "1.443375672974064\n",
      "---\n",
      "ease-of-use\n",
      "0.4166666666666667\n",
      "1.0836246694508316\n",
      "---\n",
      "utility\n",
      "-0.3333333333333333\n",
      "1.3026778945578592\n",
      "---\n",
      "overall\n",
      "0.75\n",
      "1.0552897060221726\n",
      "---\n",
      "0.2638888888888889\n",
      "1.2558937736046905\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import math\n",
    "pilot_task_dict = {\n",
    "    '1': '1. Find Extremum',\n",
    "    '2': '2. Retrieve Value',\n",
    "    '3': '3. Prediction',\n",
    "    '4': '4. Exploration'\n",
    "}\n",
    "\n",
    "path_to_json = './pilots/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('_ptask.json')]\n",
    "# print(json_files) \n",
    "jsons_data = pd.DataFrame(columns=['participant_id',\n",
    "                                   'dataset',\n",
    "                                   'oracle',\n",
    "                                   'search',\n",
    "                                   'condition',\n",
    "                                   'task',\n",
    "                                   'confidence-udata', \n",
    "                                   'confidence-ans', \n",
    "                                   'efficiency',\n",
    "                                   'ease-of-use', \n",
    "                                   'utility', \n",
    "                                   'overall'])\n",
    "\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        \n",
    "        split_filename = js.split('_')\n",
    "        \n",
    "        participant_id = split_filename[0].replace('partcipant','')\n",
    "        experimental_setup = split_filename[1]\n",
    "        dataset = dataset_dict[experimental_setup[0]]\n",
    "        \n",
    "        oracle = oracle_dict[experimental_setup[1]]\n",
    "        search = search_algorithm_dict[experimental_setup[2]]\n",
    "        \n",
    "        condition = oracle+\"_\"+search\n",
    "        \n",
    "        task = task_dict[split_filename[2]]\n",
    "        \n",
    "        confidence_udata = response_to_score_dict[json_text['confidence-udata']]\n",
    "        confidence_ans = response_to_score_dict[json_text['confidence-ans']]\n",
    "        efficiency = response_to_score_dict[json_text['efficiency']]\n",
    "        ease_of_use = response_to_score_dict[json_text['ease-of-use']]\n",
    "        utility = response_to_score_dict[json_text['utility']]\n",
    "        overall = response_to_score_dict[json_text['overall']]\n",
    "        \n",
    "        row = [participant_id, dataset, oracle, search, condition, task, confidence_udata, confidence_ans, efficiency, ease_of_use, utility, overall]\n",
    "        jsons_data.loc[index] = row\n",
    "# now that we have the pertinent json data in our DataFrame let's look at it\n",
    "print(jsons_data)\n",
    "print(\"confidence-udata\")\n",
    "print(jsons_data['confidence-udata'].mean())\n",
    "print(jsons_data['confidence-udata'].std())\n",
    "print(\"---\")\n",
    "\n",
    "print(\"confidence-ans\")\n",
    "print(jsons_data['confidence-ans'].mean())\n",
    "print(jsons_data['confidence-ans'].std())\n",
    "print(\"---\")\n",
    "\n",
    "print(\"efficiency\")\n",
    "print(jsons_data['efficiency'].mean())\n",
    "print(jsons_data['efficiency'].std())\n",
    "print(\"---\")\n",
    "\n",
    "print(\"ease-of-use\")\n",
    "print(jsons_data['ease-of-use'].mean())\n",
    "print(jsons_data['ease-of-use'].std())\n",
    "print(\"---\")\n",
    "\n",
    "print(\"utility\")\n",
    "print(jsons_data['utility'].mean())\n",
    "print(jsons_data['utility'].std())\n",
    "print(\"---\")\n",
    "\n",
    "print(\"overall\")\n",
    "print(jsons_data['overall'].mean())\n",
    "print(jsons_data['overall'].std())\n",
    "print(\"---\")\n",
    "\n",
    "\n",
    "s = pd.concat([jsons_data['confidence-udata'], \n",
    "               jsons_data['confidence-ans'], \n",
    "               jsons_data['efficiency'], \n",
    "               jsons_data['ease-of-use'],\n",
    "               jsons_data['utility'],\n",
    "               jsons_data['overall'],\n",
    "              ])\n",
    "print(s.mean())\n",
    "print(s.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    participant_id      dataset     oracle search      condition  \\\n",
      "0     partcipant41       movies     dziban    bfs     dziban_bfs   \n",
      "1      partcipant5       movies     dziban    bfs     dziban_bfs   \n",
      "2     partcipant36  birdstrikes  compassql    dfs  compassql_dfs   \n",
      "3     partcipant17       movies  compassql    dfs  compassql_dfs   \n",
      "4     partcipant56       movies     dziban    dfs     dziban_dfs   \n",
      "..             ...          ...        ...    ...            ...   \n",
      "231    partcipant9       movies     dziban    bfs     dziban_bfs   \n",
      "232   partcipant44       movies     dziban    bfs     dziban_bfs   \n",
      "233   partcipant32       movies  compassql    bfs  compassql_bfs   \n",
      "234   partcipant48  birdstrikes  compassql    bfs  compassql_bfs   \n",
      "235   partcipant14       movies  compassql    dfs  compassql_dfs   \n",
      "\n",
      "                  task     time  \n",
      "0        3. Prediction   785329  \n",
      "1       4. Exploration   595646  \n",
      "2        3. Prediction  1445670  \n",
      "3    2. Retrieve Value   103418  \n",
      "4        3. Prediction   665856  \n",
      "..                 ...      ...  \n",
      "231     4. Exploration   829866  \n",
      "232   1. Find Extremum   378562  \n",
      "233  2. Retrieve Value   522845  \n",
      "234      3. Prediction   614957  \n",
      "235     4. Exploration   630240  \n",
      "\n",
      "[236 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "path_to_json = './logs/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('_logs.json')]\n",
    "# print(json_files) \n",
    "jsons_data = pd.DataFrame(columns=['participant_id',\n",
    "                                   'dataset',\n",
    "                                   'oracle',\n",
    "                                   'search',\n",
    "                                   'condition',\n",
    "                                   'task',\n",
    "                                   'time'])\n",
    "\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        split_filename = js.split('_')\n",
    "        \n",
    "        participant_id = split_filename[0]\n",
    "        experimental_setup = split_filename[1]\n",
    "        dataset = dataset_dict[experimental_setup[0]]\n",
    "        oracle = oracle_dict[experimental_setup[1]]\n",
    "        search = search_algorithm_dict[experimental_setup[2]]\n",
    "        condition = oracle+\"_\"+search\n",
    "\n",
    "        task = task_dict[split_filename[2]]\n",
    "        time = json_text[len(json_text)-1]['Time'] - json_text[0]['Time']\n",
    "        row = [participant_id, dataset, oracle, search, condition, task, time]\n",
    "\n",
    "        jsons_data.loc[index] = row\n",
    "            \n",
    "print(jsons_data)\n",
    "jsons_data.to_csv('task_times.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   participant_id      dataset     oracle search               task     time\n",
      "16         pilot3       movies  compassql    dfs   1. Find Extremum  163.058\n",
      "10         pilot3       movies  compassql    dfs  2. Retrieve Value  407.044\n",
      "3          pilot3       movies  compassql    dfs      3. Prediction  636.623\n",
      "5          pilot3       movies  compassql    dfs     4. Exploration  387.846\n",
      "8          pilot4       movies     dziban    dfs   1. Find Extremum  110.347\n",
      "13         pilot4       movies     dziban    dfs  2. Retrieve Value  301.887\n",
      "17         pilot4       movies     dziban    dfs      3. Prediction  847.152\n",
      "11         pilot4       movies     dziban    dfs     4. Exploration  543.307\n",
      "12         pilot5  birdstrikes     dziban    bfs   1. Find Extremum  297.622\n",
      "7          pilot5  birdstrikes     dziban    bfs  2. Retrieve Value  231.168\n",
      "1          pilot5  birdstrikes     dziban    bfs      3. Prediction   83.174\n",
      "4          pilot5  birdstrikes     dziban    bfs     4. Exploration  316.487\n",
      "360.47625\n",
      "224.40410676444853\n",
      "===== 1. Find Extremum\n",
      "190.34233333333336\n",
      "96.57280952904567\n",
      "===== 2. Retrieve Value\n",
      "313.36633333333333\n",
      "88.4981534515457\n",
      "===== 3. Prediction\n",
      "522.3163333333333\n",
      "394.6075349690289\n",
      "===== 4. Exploration\n",
      "415.88000000000005\n",
      "115.97955409036543\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "import math\n",
    "pilot_task_dict = {\n",
    "    '1': '1. Find Extremum',\n",
    "    '2': '2. Retrieve Value',\n",
    "    '3': '3. Prediction',\n",
    "    '4': '4. Exploration'\n",
    "}\n",
    "\n",
    "path_to_json = './pilots/'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('_logs.json')]\n",
    "# print(json_files) \n",
    "jsons_data = pd.DataFrame(columns=['participant_id',\n",
    "                                   'dataset',\n",
    "                                   'oracle',\n",
    "                                   'search',\n",
    "                                   'task',\n",
    "                                   'time'])\n",
    "\n",
    "for index, js in enumerate(json_files):\n",
    "    with open(os.path.join(path_to_json, js)) as json_file:\n",
    "        json_text = json.load(json_file)\n",
    "        split_filename = js.split('_')\n",
    "        \n",
    "        participant_id = split_filename[0]\n",
    "        if(participant_id in ['pilot3', 'pilot4', 'pilot5']):\n",
    "            experimental_setup = split_filename[1]\n",
    "            dataset = dataset_dict[experimental_setup[0]]\n",
    "            oracle = oracle_dict[experimental_setup[1]]\n",
    "            search = search_algorithm_dict[experimental_setup[2]]\n",
    "\n",
    "            task = pilot_task_dict[split_filename[2][1]]\n",
    "\n",
    "            # time = math.log((json_text[len(json_text)-1]['Time'] - json_text[0]['Time'])/1000)\n",
    "            time = (json_text[len(json_text)-1]['Time'] - json_text[0]['Time'])/1000\n",
    "            row = [participant_id, dataset, oracle, search, task, time]\n",
    "\n",
    "            jsons_data.loc[index] = row\n",
    "jsons_data.sort_values(by=['participant_id', 'task'], inplace=True)\n",
    "print(jsons_data)\n",
    "print(jsons_data['time'].mean())\n",
    "print(jsons_data['time'].std())\n",
    "for task in ['1. Find Extremum', '2. Retrieve Value', '3. Prediction', '4. Exploration']:\n",
    "    print(\"===== \" + task)\n",
    "    df = jsons_data.loc[jsons_data['task'] == task]\n",
    "    print(df['time'].mean())\n",
    "    print(df['time'].std())\n",
    "jsons_data.to_csv('pilot_times.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './processed_completion_time_split.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-51b97b642505>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./processed_completion_time_split.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SteelSamurai/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SteelSamurai/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SteelSamurai/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SteelSamurai/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/SteelSamurai/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './processed_completion_time_split.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./processed_completion_time_split.csv\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
